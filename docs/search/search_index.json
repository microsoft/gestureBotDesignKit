{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics gestureBot Design Kit Welcome! This gestureBot Design Kit repository contains all the information needed to build both a virtual and physical desktop companion robot. Along with other tools in the Labanotation Suite, the gestureBot can be used to create and edit gestures documented in the Labanotation format and see them performed by a robot. A Windows 10 PC is required to run the software and the virtual robot. The physical robot can be built using a desktop 3D-printer and about $350(USD) in electronic servos and parts. The kit contains a browser-based simulated robot with control software based on the Robotis XL series of servo motors. To construct a physical robot, it provides models for 3D-printable body-parts, a parts-list and purchase links for electronic components, and step-by-step assembly instructions. No soldering is required, but some manual skill is needed to mate small electronic connectors, manipulate small plastic rivets, and install miniature metal screws to assemble the body components. What is the gestureBot and who might be interested in using this kit? The gestureBot is a semi-humanoid service-robot capable of mimicking human gestures. Its purpose is to support research and education in the field of Human-Robot-Interaction (HRI) . Its function is to physically and virtually render gestures stored in a digital Labanotation format. The designation \"semi-humanoid\" means that it has some but not all of the limbs and joints of a human form. The designation \"service-robot\" refers to a requirement that is designed to safely operate directly with humans, as opposed to an \"industrial-robot\" that usually repeats tasks within a cordoned-off safety zone. The gestureBot was adapted from a project originally created by interns working for Microsoft Research Asia. We've built on their work and put together this design kit to help students learn with a fun hand's-on project that provides exposure to key technologies used in service-robotics. Researchers in the field of HRI can use the kit to support experimentation in the use of gestures to enhance robot-to-human comunication. No matter where your interest lies, we hope you find the gestureBot Design Kit a fun and easy way to work and learn in the field of robotics. How It Works System Diagram System Modules: Gesture Service Labanotation Controller The Labanotation Controller reads .json files containing gestures described in Labanotation format, which are recordings of human gestural movements reduced to significant keyframes consisting of sequenced joint position and movement timing goals. Gesture selection and loading, as well as joint positional movements and their durations contained in the gestures, are requested by and sent to the gestureBot Controller module. The keyframes can also be transmitted under the control of simple play/pause commands and a scrubbing timeline with a browser-based UI available on http port 8000 . Gesture Library The Gesture Library is a database of gesture-concept pairs expressed in Labanotation format and stored as .json files. The data is directly accessed by the Gesture Engine. The library includes a complete listing of the sample database including a video clip of each gesture performed by the gestureBot. Gesture Engine For humanoid robot performance, the Gesture Engine contains a software module constructed with Python and Google's neural network word2vec . It receives text phrases from the Labanotation Controller as input and returns corresponding gestures. Sample text phrases and gesture-concept pairs contained in the Gesture Library can be previewed by the system with a browser-based UI available on http port 8002 . gestureBot Controller The gestureBot Controller requests from the Labanotation Controller gestures to be loaded and played. Optionally, it will interpolate and insert interstitial keyframes between Labanotation keyframes in a gesture at a selectable sampling rate. It transforms goal positions and times contained in the keyframes into joint servo commands calculated using information specific to the the gestureBot's physical characteristics. It animates both the real and models of the robot using the servo commands and a library of virtual model assets. Finally, it provides a browser-based user interface on http port 8001 for: * Selecting a Gesture-Concept .json input file for rendering * Optionally setting a sample rate for interstitial keyframe extrapolations between Labanotation keyframes * Optionally showing keyframe goal positions with \"bone segments\" within the browser-hosted model * Optionally hide cover components in the browser-hosted model * Direct override control for individual servos in the gestureBot * Previewing a gesture or movement in a simple simulator * Reporting application status with console messages Real and Simulated gestureBots Both physical gestureBots and virtual models hosted in a browser connect to the gestureBot controller and receive movement commands as servo-control packets. The packets adhere to the Robotis Dynamixel 2.0 Protocol . A packet is sent for each Labanotation and interstitial keyframe transmitted by the gestureBot Controller and contains individual goal positions and movement durations for each servo in the gestureBot. Install, Build and Run Software Installation Instructions for the controller applications Step-by-Step Instructions for constructing a physical gestureBot Contributing We think the gestureBot can be made to do much more than render gestures and we hope you will help it evolve! We have designed the hardware and software to be easy to understand and modify. - Please read our short and sweet coding guidelines . - For big changes such as adding new feature or refactoring, file an issue first . - Use our recommended development workflow to make changes and test it. - Use usual steps to make contributions just like other GitHub projects. If you are not familiar with Git Branch-Rebase-Merge workflow, please read this first . Checklist Use same style and formatting as rest of code even if it's not your preferred one. Change any documentation that goes with code changes. Do not include OS specific header files or new 3rd party dependencies. Keep your pull request small, ideally under 10 files. Don't don't include large binary files. When adding new includes, make sure the dependency is absolutely necessary. Rebase your branch frequently with master (once every 2-3 days is ideal). Make sure your code compiles on Windows, Linux and OSX. FAQ Q Who should I contact regarding this repository? A Please create a Github issue or email robosupp@microsoft.com with any questions or feedback. Q How challenging is this project for children? A We think it depends on the child and how much support they receive! We'd love to attract kids who love Minecraft and/or Legos, but we've shaped this project to be easily accessible to high-school and college STEM faculty looking for affordable lab projects, academic HRI researchers, and casual hobbyists. Q How does this project relate to Artificial Intelligence (AI)? A Physics-based simulation of real robots in virtual environments opens the door for the use of AI \"brain\" training using Reinforcement Learning tool-chains such as Microsoft Bonsai . Citation If you want to cite this work, please use the following bibtex code @Article{Ikeuchi2018, author= Ikeuchi, Katsushi and Ma, Zhaoyuan and Yan, Zengqiang and Kudoh, Shunsuke and Nakamura, Minako , title= Describing Upper-Body Motions Based on Labanotation for Learning-from-Observation Robots , journal= International Journal of Computer Vision , year= 2018 , month= Dec , day= 01 , volume= 126 , number= 12 , pages= 1415--1429 , issn= 1573-1405 , doi= 10.1007/s11263-018-1123-1 , url= https://doi.org/10.1007/s11263-018-1123-1 } If you want to learn more about creating good readme files then refer the following guidelines . You can also seek inspiration from the below readme files: - ASP.NET Core - Visual Studio Code - Chakra Core","title":"Home"},{"location":"#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"#gesturebot-design-kit","text":"Welcome! This gestureBot Design Kit repository contains all the information needed to build both a virtual and physical desktop companion robot. Along with other tools in the Labanotation Suite, the gestureBot can be used to create and edit gestures documented in the Labanotation format and see them performed by a robot. A Windows 10 PC is required to run the software and the virtual robot. The physical robot can be built using a desktop 3D-printer and about $350(USD) in electronic servos and parts. The kit contains a browser-based simulated robot with control software based on the Robotis XL series of servo motors. To construct a physical robot, it provides models for 3D-printable body-parts, a parts-list and purchase links for electronic components, and step-by-step assembly instructions. No soldering is required, but some manual skill is needed to mate small electronic connectors, manipulate small plastic rivets, and install miniature metal screws to assemble the body components.","title":"gestureBot Design Kit"},{"location":"#what-is-the-gesturebot-and-who-might-be-interested-in-using-this-kit","text":"The gestureBot is a semi-humanoid service-robot capable of mimicking human gestures. Its purpose is to support research and education in the field of Human-Robot-Interaction (HRI) . Its function is to physically and virtually render gestures stored in a digital Labanotation format. The designation \"semi-humanoid\" means that it has some but not all of the limbs and joints of a human form. The designation \"service-robot\" refers to a requirement that is designed to safely operate directly with humans, as opposed to an \"industrial-robot\" that usually repeats tasks within a cordoned-off safety zone. The gestureBot was adapted from a project originally created by interns working for Microsoft Research Asia. We've built on their work and put together this design kit to help students learn with a fun hand's-on project that provides exposure to key technologies used in service-robotics. Researchers in the field of HRI can use the kit to support experimentation in the use of gestures to enhance robot-to-human comunication. No matter where your interest lies, we hope you find the gestureBot Design Kit a fun and easy way to work and learn in the field of robotics.","title":"What is the gestureBot and who might be interested in using this kit?"},{"location":"#how-it-works","text":"","title":"How It Works"},{"location":"#system-diagram","text":"","title":"System Diagram"},{"location":"#system-modules","text":"","title":"System Modules:"},{"location":"#gesture-service","text":"","title":"Gesture Service"},{"location":"#labanotation-controller","text":"The Labanotation Controller reads .json files containing gestures described in Labanotation format, which are recordings of human gestural movements reduced to significant keyframes consisting of sequenced joint position and movement timing goals. Gesture selection and loading, as well as joint positional movements and their durations contained in the gestures, are requested by and sent to the gestureBot Controller module. The keyframes can also be transmitted under the control of simple play/pause commands and a scrubbing timeline with a browser-based UI available on http port 8000 .","title":"Labanotation Controller"},{"location":"#gesture-library","text":"The Gesture Library is a database of gesture-concept pairs expressed in Labanotation format and stored as .json files. The data is directly accessed by the Gesture Engine. The library includes a complete listing of the sample database including a video clip of each gesture performed by the gestureBot.","title":"Gesture Library"},{"location":"#gesture-engine","text":"For humanoid robot performance, the Gesture Engine contains a software module constructed with Python and Google's neural network word2vec . It receives text phrases from the Labanotation Controller as input and returns corresponding gestures. Sample text phrases and gesture-concept pairs contained in the Gesture Library can be previewed by the system with a browser-based UI available on http port 8002 .","title":"Gesture Engine"},{"location":"#gesturebot-controller","text":"The gestureBot Controller requests from the Labanotation Controller gestures to be loaded and played. Optionally, it will interpolate and insert interstitial keyframes between Labanotation keyframes in a gesture at a selectable sampling rate. It transforms goal positions and times contained in the keyframes into joint servo commands calculated using information specific to the the gestureBot's physical characteristics. It animates both the real and models of the robot using the servo commands and a library of virtual model assets. Finally, it provides a browser-based user interface on http port 8001 for: * Selecting a Gesture-Concept .json input file for rendering * Optionally setting a sample rate for interstitial keyframe extrapolations between Labanotation keyframes * Optionally showing keyframe goal positions with \"bone segments\" within the browser-hosted model * Optionally hide cover components in the browser-hosted model * Direct override control for individual servos in the gestureBot * Previewing a gesture or movement in a simple simulator * Reporting application status with console messages","title":"gestureBot Controller"},{"location":"#real-and-simulated-gesturebots","text":"Both physical gestureBots and virtual models hosted in a browser connect to the gestureBot controller and receive movement commands as servo-control packets. The packets adhere to the Robotis Dynamixel 2.0 Protocol . A packet is sent for each Labanotation and interstitial keyframe transmitted by the gestureBot Controller and contains individual goal positions and movement durations for each servo in the gestureBot.","title":"Real and Simulated gestureBots"},{"location":"#install-build-and-run","text":"Software Installation Instructions for the controller applications Step-by-Step Instructions for constructing a physical gestureBot","title":"Install, Build and Run"},{"location":"#contributing","text":"We think the gestureBot can be made to do much more than render gestures and we hope you will help it evolve! We have designed the hardware and software to be easy to understand and modify. - Please read our short and sweet coding guidelines . - For big changes such as adding new feature or refactoring, file an issue first . - Use our recommended development workflow to make changes and test it. - Use usual steps to make contributions just like other GitHub projects. If you are not familiar with Git Branch-Rebase-Merge workflow, please read this first .","title":"Contributing"},{"location":"#checklist","text":"Use same style and formatting as rest of code even if it's not your preferred one. Change any documentation that goes with code changes. Do not include OS specific header files or new 3rd party dependencies. Keep your pull request small, ideally under 10 files. Don't don't include large binary files. When adding new includes, make sure the dependency is absolutely necessary. Rebase your branch frequently with master (once every 2-3 days is ideal). Make sure your code compiles on Windows, Linux and OSX.","title":"Checklist"},{"location":"#faq","text":"Q Who should I contact regarding this repository? A Please create a Github issue or email robosupp@microsoft.com with any questions or feedback. Q How challenging is this project for children? A We think it depends on the child and how much support they receive! We'd love to attract kids who love Minecraft and/or Legos, but we've shaped this project to be easily accessible to high-school and college STEM faculty looking for affordable lab projects, academic HRI researchers, and casual hobbyists. Q How does this project relate to Artificial Intelligence (AI)? A Physics-based simulation of real robots in virtual environments opens the door for the use of AI \"brain\" training using Reinforcement Learning tool-chains such as Microsoft Bonsai .","title":"FAQ"},{"location":"#citation","text":"If you want to cite this work, please use the following bibtex code @Article{Ikeuchi2018, author= Ikeuchi, Katsushi and Ma, Zhaoyuan and Yan, Zengqiang and Kudoh, Shunsuke and Nakamura, Minako , title= Describing Upper-Body Motions Based on Labanotation for Learning-from-Observation Robots , journal= International Journal of Computer Vision , year= 2018 , month= Dec , day= 01 , volume= 126 , number= 12 , pages= 1415--1429 , issn= 1573-1405 , doi= 10.1007/s11263-018-1123-1 , url= https://doi.org/10.1007/s11263-018-1123-1 } If you want to learn more about creating good readme files then refer the following guidelines . You can also seek inspiration from the below readme files: - ASP.NET Core - Visual Studio Code - Chakra Core","title":"Citation"},{"location":"CODE_OF_CONDUCT/","text":"Microsoft Open Source Code of Conduct This project has adopted the Microsoft Open Source Code of Conduct . Resources: Microsoft Open Source Code of Conduct Microsoft Code of Conduct FAQ Contact opencode@microsoft.com with questions or concerns","title":"Microsoft Open Source Code of Conduct"},{"location":"CODE_OF_CONDUCT/#microsoft-open-source-code-of-conduct","text":"This project has adopted the Microsoft Open Source Code of Conduct . Resources: Microsoft Open Source Code of Conduct Microsoft Code of Conduct FAQ Contact opencode@microsoft.com with questions or concerns","title":"Microsoft Open Source Code of Conduct"},{"location":"SECURITY/","text":"Security Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include Microsoft , Azure , DotNet , AspNet , Xamarin , and our GitHub organizations . If you believe you have found a security vulnerability in any Microsoft-owned repository that meets Microsoft's definition of a security vulnerability , please report it to us as described below. Reporting Security Issues Please do not report security vulnerabilities through public GitHub issues. Instead, please report them to the Microsoft Security Response Center (MSRC) at https://msrc.microsoft.com/create-report . If you prefer to submit without logging in, send email to secure@microsoft.com . If possible, encrypt your message with our PGP key; please download it from the Microsoft Security Response Center PGP Key page . You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at microsoft.com/msrc . Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue: Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.) Full paths of source file(s) related to the manifestation of the issue The location of the affected source code (tag/branch/commit or direct URL) Any special configuration required to reproduce the issue Step-by-step instructions to reproduce the issue Proof-of-concept or exploit code (if possible) Impact of the issue, including how an attacker might exploit the issue This information will help us triage your report more quickly. If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our Microsoft Bug Bounty Program page for more details about our active programs. Preferred Languages We prefer all communications to be in English. Policy Microsoft follows the principle of Coordinated Vulnerability Disclosure .","title":"SECURITY"},{"location":"SECURITY/#security","text":"Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include Microsoft , Azure , DotNet , AspNet , Xamarin , and our GitHub organizations . If you believe you have found a security vulnerability in any Microsoft-owned repository that meets Microsoft's definition of a security vulnerability , please report it to us as described below.","title":"Security"},{"location":"SECURITY/#reporting-security-issues","text":"Please do not report security vulnerabilities through public GitHub issues. Instead, please report them to the Microsoft Security Response Center (MSRC) at https://msrc.microsoft.com/create-report . If you prefer to submit without logging in, send email to secure@microsoft.com . If possible, encrypt your message with our PGP key; please download it from the Microsoft Security Response Center PGP Key page . You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at microsoft.com/msrc . Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue: Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.) Full paths of source file(s) related to the manifestation of the issue The location of the affected source code (tag/branch/commit or direct URL) Any special configuration required to reproduce the issue Step-by-step instructions to reproduce the issue Proof-of-concept or exploit code (if possible) Impact of the issue, including how an attacker might exploit the issue This information will help us triage your report more quickly. If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our Microsoft Bug Bounty Program page for more details about our active programs.","title":"Reporting Security Issues"},{"location":"SECURITY/#preferred-languages","text":"We prefer all communications to be in English.","title":"Preferred Languages"},{"location":"SECURITY/#policy","text":"Microsoft follows the principle of Coordinated Vulnerability Disclosure .","title":"Policy"},{"location":"SUPPORT/","text":"TODO: The maintainer of this repo has not yet edited this file REPO OWNER : Do you want Customer Service Support (CSS) support for this product/project? No CSS support: Fill out this template with information about how to file issues and get help. Yes CSS support: Fill out an intake form at aka.ms/spot . CSS will work with/help you to determine next steps. More details also available at aka.ms/onboardsupport . Not sure? Fill out a SPOT intake as though the answer were \"Yes\". CSS will help you decide. Then remove this first heading from this SUPPORT.MD file before publishing your repo. Support How to file issues and get help This project uses GitHub Issues to track bugs and feature requests. Please search the existing issues before filing new issues to avoid duplicates. For new issues, file your bug or feature request as a new Issue. For help and questions about using this project, please REPO MAINTAINER: INSERT INSTRUCTIONS HERE FOR HOW TO ENGAGE REPO OWNERS OR COMMUNITY FOR HELP. COULD BE A STACK OVERFLOW TAG OR OTHER CHANNEL. WHERE WILL YOU HELP PEOPLE? . Microsoft Support Policy Support for this PROJECT or PRODUCT is limited to the resources listed above.","title":"TODO: The maintainer of this repo has not yet edited this file"},{"location":"SUPPORT/#todo-the-maintainer-of-this-repo-has-not-yet-edited-this-file","text":"REPO OWNER : Do you want Customer Service Support (CSS) support for this product/project? No CSS support: Fill out this template with information about how to file issues and get help. Yes CSS support: Fill out an intake form at aka.ms/spot . CSS will work with/help you to determine next steps. More details also available at aka.ms/onboardsupport . Not sure? Fill out a SPOT intake as though the answer were \"Yes\". CSS will help you decide. Then remove this first heading from this SUPPORT.MD file before publishing your repo.","title":"TODO: The maintainer of this repo has not yet edited this file"},{"location":"SUPPORT/#support","text":"","title":"Support"},{"location":"SUPPORT/#how-to-file-issues-and-get-help","text":"This project uses GitHub Issues to track bugs and feature requests. Please search the existing issues before filing new issues to avoid duplicates. For new issues, file your bug or feature request as a new Issue. For help and questions about using this project, please REPO MAINTAINER: INSERT INSTRUCTIONS HERE FOR HOW TO ENGAGE REPO OWNERS OR COMMUNITY FOR HELP. COULD BE A STACK OVERFLOW TAG OR OTHER CHANNEL. WHERE WILL YOU HELP PEOPLE? .","title":"How to file issues and get help"},{"location":"SUPPORT/#microsoft-support-policy","text":"Support for this PROJECT or PRODUCT is limited to the resources listed above.","title":"Microsoft Support Policy"},{"location":"coding_guidelines/","text":"Modern Python Coding Guidelines Runs under Python3 3-space indents Modern C++ Coding Guidelines We are using Modern C++11. Smart pointers, Lambdas, and C++11 multithreading primitives are your friend. Quick Note The great thing about \"standards\" is that there are many to chose from: ISO , Sutter Stroustrup , ROS , LINUX , Google's , Microsoft's , CERN's , GCC's , ARM's , LLVM's and probably thousands of others. Unfortunately most of these can't even agree on something as basic as how to name a class or a constant. This is probably due to the fact that these standards often carry lots of legacy issues due to supporting existing code bases. The intention behind this document is to create guidance that remains as close to ISO, Sutter Stroustrup and ROS while resolving as many conflicts, disadvantages and inconsistencies as possible among them. Naming Conventions Avoid using any sort of Hungarian notation on names and \"_ptr\" on pointers. Code Element Style Comment Namespace under_scored Differentiate from class names Class name CamelCase To differentiate from STL types which ISO recommends (do not use \"C\" or \"T\" prefixes) Function name camelCase Lower case start is almost universal except for .Net world Parameters/Locals under_scored Vast majority of standards recommends this because _ is more readable to C++ crowd (although not much to Java/.Net crowd) Member variables under_scored_with_ The prefix _ is heavily discouraged as ISO has rules around reserving _identifiers, so we recommend suffix instead Enums and its members CamelCase Most except very old standards agree with this one Globals g_under_scored You shouldn't have these in first place! Constants UPPER_CASE Very contentious and we just have to pick one here, unless if is a private constant in class or method, then use naming for Members or Locals File names Match case of class name in file Lot of pro and cons either way but this removes inconsistency in auto generated code (important for ROS) Header Files Use a namespace qualified #ifdef to protect against multiple inclusion: #ifndef msr_airsim_MyHeader_hpp #define msr_airsim_MyHeader_hpp //--your code #endif The reason we don't use #pragma once is because it's not supported if same header file exists at multiple places (which might be possible under ROS build system!). Bracketing Inside function or method body place curly bracket on same line. Outside that the Namespace, Class and methods levels use separate line. This is called K R style and its variants are widely used in C++ vs other styles which are more popular in other languages. Notice that curlies are not required if you have single statement, but complex statements are easier to keep correct with the braces. int main(int argc, char* argv[]) { while (x == y) { f0(); if (cont()) { f1(); } else { f2(); f3(); } if (x 100) break; } } Const and References Religiously review all non-scalar parameters you declare to be candidate for const and references. If you are coming from languages such as C#/Java/Python, the most often mistake you would make is to pass parameters by value instead of const T ; Especially most of the strings, vectors and maps you want to pass as const T ; (if they are readonly) or T (if they are writable). Also add const suffix to methods as much as possible. Overriding When overriding virtual method, use override suffix. Pointers This is really about memory management. A simulator has much performance critical code, so we try and avoid overloading the memory manager with lots of calls to new/delete. We also want to avoid too much copying of things on the stack, so we pass things by reference when ever possible. But when the object really needs to live longer than the call stack you often need to allocate that object on the heap, and so you have a pointer. Now, if management of the lifetime of that object is going to be tricky we recommend using C++ 11 smart pointers . But smart pointers do have a cost, so don\u2019t use them blindly everywhere. For private code where performance is paramount, raw pointers can be used. Raw pointers are also often needed when interfacing with legacy systems that only accept pointer types, for example, sockets API. But we try to wrap those legacy interfaces as much as possible and avoid that style of programming from leaking into the larger code base. Religiously check if you can use const everywhere, for example, const float * const xP . Avoid using prefix or suffix to indicate pointer types in variable names, i.e. use my_obj instead of myobj_ptr except in cases where it might make sense to differentiate variables better, for example, int mynum = 5; int* mynum_ptr = mynum; This is Too Short, ye? Yes, and it's on purpose because no one likes to read 200 page coding guidelines. The goal here is to cover only most significant things which are already not covered by strict mode compilation in GCC and Level 4 warnings-as-errors in VC++. If you had like to know about how to write better code in C++, please see GotW and Effective Modern C++ book.","title":"Modern Python Coding Guidelines"},{"location":"coding_guidelines/#modern-python-coding-guidelines","text":"Runs under Python3 3-space indents","title":"Modern Python Coding Guidelines"},{"location":"coding_guidelines/#modern-c-coding-guidelines","text":"We are using Modern C++11. Smart pointers, Lambdas, and C++11 multithreading primitives are your friend.","title":"Modern C++ Coding Guidelines"},{"location":"coding_guidelines/#quick-note","text":"The great thing about \"standards\" is that there are many to chose from: ISO , Sutter Stroustrup , ROS , LINUX , Google's , Microsoft's , CERN's , GCC's , ARM's , LLVM's and probably thousands of others. Unfortunately most of these can't even agree on something as basic as how to name a class or a constant. This is probably due to the fact that these standards often carry lots of legacy issues due to supporting existing code bases. The intention behind this document is to create guidance that remains as close to ISO, Sutter Stroustrup and ROS while resolving as many conflicts, disadvantages and inconsistencies as possible among them.","title":"Quick Note"},{"location":"coding_guidelines/#naming-conventions","text":"Avoid using any sort of Hungarian notation on names and \"_ptr\" on pointers. Code Element Style Comment Namespace under_scored Differentiate from class names Class name CamelCase To differentiate from STL types which ISO recommends (do not use \"C\" or \"T\" prefixes) Function name camelCase Lower case start is almost universal except for .Net world Parameters/Locals under_scored Vast majority of standards recommends this because _ is more readable to C++ crowd (although not much to Java/.Net crowd) Member variables under_scored_with_ The prefix _ is heavily discouraged as ISO has rules around reserving _identifiers, so we recommend suffix instead Enums and its members CamelCase Most except very old standards agree with this one Globals g_under_scored You shouldn't have these in first place! Constants UPPER_CASE Very contentious and we just have to pick one here, unless if is a private constant in class or method, then use naming for Members or Locals File names Match case of class name in file Lot of pro and cons either way but this removes inconsistency in auto generated code (important for ROS)","title":"Naming Conventions"},{"location":"coding_guidelines/#header-files","text":"Use a namespace qualified #ifdef to protect against multiple inclusion: #ifndef msr_airsim_MyHeader_hpp #define msr_airsim_MyHeader_hpp //--your code #endif The reason we don't use #pragma once is because it's not supported if same header file exists at multiple places (which might be possible under ROS build system!).","title":"Header Files"},{"location":"coding_guidelines/#bracketing","text":"Inside function or method body place curly bracket on same line. Outside that the Namespace, Class and methods levels use separate line. This is called K R style and its variants are widely used in C++ vs other styles which are more popular in other languages. Notice that curlies are not required if you have single statement, but complex statements are easier to keep correct with the braces. int main(int argc, char* argv[]) { while (x == y) { f0(); if (cont()) { f1(); } else { f2(); f3(); } if (x 100) break; } }","title":"Bracketing"},{"location":"coding_guidelines/#const-and-references","text":"Religiously review all non-scalar parameters you declare to be candidate for const and references. If you are coming from languages such as C#/Java/Python, the most often mistake you would make is to pass parameters by value instead of const T ; Especially most of the strings, vectors and maps you want to pass as const T ; (if they are readonly) or T (if they are writable). Also add const suffix to methods as much as possible.","title":"Const and References"},{"location":"coding_guidelines/#overriding","text":"When overriding virtual method, use override suffix.","title":"Overriding"},{"location":"coding_guidelines/#pointers","text":"This is really about memory management. A simulator has much performance critical code, so we try and avoid overloading the memory manager with lots of calls to new/delete. We also want to avoid too much copying of things on the stack, so we pass things by reference when ever possible. But when the object really needs to live longer than the call stack you often need to allocate that object on the heap, and so you have a pointer. Now, if management of the lifetime of that object is going to be tricky we recommend using C++ 11 smart pointers . But smart pointers do have a cost, so don\u2019t use them blindly everywhere. For private code where performance is paramount, raw pointers can be used. Raw pointers are also often needed when interfacing with legacy systems that only accept pointer types, for example, sockets API. But we try to wrap those legacy interfaces as much as possible and avoid that style of programming from leaking into the larger code base. Religiously check if you can use const everywhere, for example, const float * const xP . Avoid using prefix or suffix to indicate pointer types in variable names, i.e. use my_obj instead of myobj_ptr except in cases where it might make sense to differentiate variables better, for example, int mynum = 5; int* mynum_ptr = mynum;","title":"Pointers"},{"location":"coding_guidelines/#this-is-too-short-ye","text":"Yes, and it's on purpose because no one likes to read 200 page coding guidelines. The goal here is to cover only most significant things which are already not covered by strict mode compilation in GCC and Level 4 warnings-as-errors in VC++. If you had like to know about how to write better code in C++, please see GotW and Effective Modern C++ book.","title":"This is Too Short, ye?"},{"location":"dev_workflow/","text":"Development Workflow Below is the guide on how to perform different development activities while working with AirSim. If you are new to Unreal Engine based projects and want to contribute to AirSim or make your own forks for your custom requirements, this might save you some time. Development Environment OS We highly recommend Windows 10 and Visual Studio 2019 as your development environment. The support for other OSes and IDE is unfortunately not as mature on the Unreal Engine side and you may risk severe loss of productivity trying to do workarounds and jumping through the hoops. Hardware We recommend GPUs such as NVidia 1080 or NVidia Titan series with powerful desktop such as one with 64GB RAM, 6+ cores, SSDs and 2-3 displays (ideally 4K). We have found HP Z840 work quite well for our needs. The development experience on high-end laptops is generally sub-par compared to powerful desktops however they might be useful in a pinch. You generally want laptops with discrete NVidia GPU (at least M2000 or better) with 64GB RAM, SSDs and hopefully 4K display. We have found models such as Lenovo P50 work well for our needs. Laptops with only integrated graphics might not work well. Updating and Changing AirSim Code Overview AirSim is designed as plugin. This means it can't run by itself, you need to put it in an Unreal project (we call it \"environment\"). So building and testing AirSim has two steps: (1) build the plugin (2) deploy plugin in Unreal project and run the project. The first step is accomplished by build.cmd available in AirSim root. This command will update everything you need for the plugin in the Unreal\\Plugins folder. So to deploy the plugin, you just need to copy Unreal\\Plugins folder in to your Unreal project folder. Next you should remove all intermediate files in your Unreal project and then regenerate .sln file for your Unreal project. To do this, we have two handy .bat files in Unreal\\Environments\\Blocks folder: clean.bat and GenerateProjectFiles.bat . So just run these bat files in sequence from root of your Unreal project. Now you are ready to open new .sln in Visual Studio and press F5 to run it. Steps Below are the steps we use to make changes in AirSim and test them out. The best way to do development in AirSim code is to use Blocks project . This is the light weight project so compile time is relatively faster. Generally the workflow is, REM //Use x64 Native Tools Command Prompt for VS 2019 REM //Navigate to AirSim repo folder git pull build.cmd cd Unreal\\Environments\\Blocks update_from_git.bat start Blocks.sln Above commands first builds the AirSim plugin and then deploys it to Blocks project using handy update_from_git.bat . Now you can work inside Visual Studio solution, make changes to the code and just run F5 to build, run and test your changes. The debugging, break points etc should work as usual. After you are done with you code changes, you might want to push your changes back to AirSim repo or your own fork or you may deploy the new plugin to your custom Unreal project. To do this, go back to command prompt and first update the AirSim repo folder: REM //Use x64 Native Tools Command Prompt for VS 2019 REM //run this from Unreal\\Environments\\Blocks update_to_git.bat build.cmd Above command will transfer your code changes from Unreal project folder back to Unreal\\Plugins folder. Now your changes are ready to be pushed to AirSim repo or your own fork. You can also copy Unreal\\Plugins to your custom Unreal engine project and see if everything works in your custom project as well. Take Away Once you understand how Unreal Build system and plugin model works as well as why we are doing above steps, you should feel quite comfortable in following this workflow. Don't be afraid of opening up .bat files to peek inside and see what its doing. They are quite minimal and straightforward (except, of course, build.cmd - don't look in to that one). FAQ I made changes in code in Blocks project but its not working. When you press F5 or F6 in Visual Studio to start build, the Unreal Build system kicks in and it tries to find out if any files are dirty and what it needs to build. Unfortunately, it often fails to recognize dirty files that is not the code that uses Unreal headers and object hierarchy. So, the trick is to just make some file dirty that Unreal Build system always recognizes. My favorite one is AirSimGameMode.cpp. Just insert a line, delete it and save the file. I made changes in the code outside of Visual Studio but its not working. Don't do that! Unreal Build system assumes that you are using Visual Studio and it does bunch of things to integrate with Visual Studio. If you do insist on using other editors then look up how to do command line builds in Unreal projects OR see docs on your editor on how it can integrate with Unreal build system OR run clean.bat + GenerateProjectFiles.bat to make sure VS solution is in sync. I'm trying to add new file in the Unreal Project and its not working. It won't! While you are indeed using Visual Studio solution, remember that this solution was actually generated by Unreal Build system. If you want to add new files in your project, first shut down Visual Studio, add an empty file at desired location and then run GenerateProjectFiles.bat which will scan all files in your project and then re-create the .sln file. Now open this new .sln file and you are in business. I copied Unreal\\Plugins folder but nothing happens in Unreal Project. First make sure your project's .uproject file is referencing the plugin. Then make sure you have run clean.bat and then GenerateProjectFiles.bat as described in Overview above. I have multiple Unreal projects with AirSim plugin. How do I update them easily? You are in luck! We have build_all_ue_projects.bat which exactly does that. Don't treat it as black box (at least not yet), open it up and see what it does. It has 4 variables that are being set from command line args. If these args is not supplied they are set to default values in next set of statements. You might want to change default values for the paths. This batch file builds AirSim plugin, deploys it to all listed projects (see CALL statements later in the batch file), runs packaging for those projects and puts final binaries in specified folder - all in one step! This is what we use to create our own binary releases. How do I contribute back to AirSim? Before making any changes make sure you have created your feature branch. After you test your code changes in Blocks environment, follow the usual steps to make contributions just like any other GitHub projects. If you are not familiar with Git Branch-Rebase-Merge workflow, please read this first .","title":"Development Workflow"},{"location":"dev_workflow/#development-workflow","text":"Below is the guide on how to perform different development activities while working with AirSim. If you are new to Unreal Engine based projects and want to contribute to AirSim or make your own forks for your custom requirements, this might save you some time.","title":"Development Workflow"},{"location":"dev_workflow/#development-environment","text":"","title":"Development Environment"},{"location":"dev_workflow/#os","text":"We highly recommend Windows 10 and Visual Studio 2019 as your development environment. The support for other OSes and IDE is unfortunately not as mature on the Unreal Engine side and you may risk severe loss of productivity trying to do workarounds and jumping through the hoops.","title":"OS"},{"location":"dev_workflow/#hardware","text":"We recommend GPUs such as NVidia 1080 or NVidia Titan series with powerful desktop such as one with 64GB RAM, 6+ cores, SSDs and 2-3 displays (ideally 4K). We have found HP Z840 work quite well for our needs. The development experience on high-end laptops is generally sub-par compared to powerful desktops however they might be useful in a pinch. You generally want laptops with discrete NVidia GPU (at least M2000 or better) with 64GB RAM, SSDs and hopefully 4K display. We have found models such as Lenovo P50 work well for our needs. Laptops with only integrated graphics might not work well.","title":"Hardware"},{"location":"dev_workflow/#updating-and-changing-airsim-code","text":"","title":"Updating and Changing AirSim Code"},{"location":"dev_workflow/#overview","text":"AirSim is designed as plugin. This means it can't run by itself, you need to put it in an Unreal project (we call it \"environment\"). So building and testing AirSim has two steps: (1) build the plugin (2) deploy plugin in Unreal project and run the project. The first step is accomplished by build.cmd available in AirSim root. This command will update everything you need for the plugin in the Unreal\\Plugins folder. So to deploy the plugin, you just need to copy Unreal\\Plugins folder in to your Unreal project folder. Next you should remove all intermediate files in your Unreal project and then regenerate .sln file for your Unreal project. To do this, we have two handy .bat files in Unreal\\Environments\\Blocks folder: clean.bat and GenerateProjectFiles.bat . So just run these bat files in sequence from root of your Unreal project. Now you are ready to open new .sln in Visual Studio and press F5 to run it.","title":"Overview"},{"location":"dev_workflow/#steps","text":"Below are the steps we use to make changes in AirSim and test them out. The best way to do development in AirSim code is to use Blocks project . This is the light weight project so compile time is relatively faster. Generally the workflow is, REM //Use x64 Native Tools Command Prompt for VS 2019 REM //Navigate to AirSim repo folder git pull build.cmd cd Unreal\\Environments\\Blocks update_from_git.bat start Blocks.sln Above commands first builds the AirSim plugin and then deploys it to Blocks project using handy update_from_git.bat . Now you can work inside Visual Studio solution, make changes to the code and just run F5 to build, run and test your changes. The debugging, break points etc should work as usual. After you are done with you code changes, you might want to push your changes back to AirSim repo or your own fork or you may deploy the new plugin to your custom Unreal project. To do this, go back to command prompt and first update the AirSim repo folder: REM //Use x64 Native Tools Command Prompt for VS 2019 REM //run this from Unreal\\Environments\\Blocks update_to_git.bat build.cmd Above command will transfer your code changes from Unreal project folder back to Unreal\\Plugins folder. Now your changes are ready to be pushed to AirSim repo or your own fork. You can also copy Unreal\\Plugins to your custom Unreal engine project and see if everything works in your custom project as well.","title":"Steps"},{"location":"dev_workflow/#take-away","text":"Once you understand how Unreal Build system and plugin model works as well as why we are doing above steps, you should feel quite comfortable in following this workflow. Don't be afraid of opening up .bat files to peek inside and see what its doing. They are quite minimal and straightforward (except, of course, build.cmd - don't look in to that one).","title":"Take Away"},{"location":"dev_workflow/#faq","text":"","title":"FAQ"},{"location":"dev_workflow/#i-made-changes-in-code-in-blocks-project-but-its-not-working","text":"When you press F5 or F6 in Visual Studio to start build, the Unreal Build system kicks in and it tries to find out if any files are dirty and what it needs to build. Unfortunately, it often fails to recognize dirty files that is not the code that uses Unreal headers and object hierarchy. So, the trick is to just make some file dirty that Unreal Build system always recognizes. My favorite one is AirSimGameMode.cpp. Just insert a line, delete it and save the file.","title":"I made changes in code in Blocks project but its not working."},{"location":"dev_workflow/#i-made-changes-in-the-code-outside-of-visual-studio-but-its-not-working","text":"Don't do that! Unreal Build system assumes that you are using Visual Studio and it does bunch of things to integrate with Visual Studio. If you do insist on using other editors then look up how to do command line builds in Unreal projects OR see docs on your editor on how it can integrate with Unreal build system OR run clean.bat + GenerateProjectFiles.bat to make sure VS solution is in sync.","title":"I made changes in the code outside of Visual Studio but its not working."},{"location":"dev_workflow/#im-trying-to-add-new-file-in-the-unreal-project-and-its-not-working","text":"It won't! While you are indeed using Visual Studio solution, remember that this solution was actually generated by Unreal Build system. If you want to add new files in your project, first shut down Visual Studio, add an empty file at desired location and then run GenerateProjectFiles.bat which will scan all files in your project and then re-create the .sln file. Now open this new .sln file and you are in business.","title":"I'm trying to add new file in the Unreal Project and its not working."},{"location":"dev_workflow/#i-copied-unrealplugins-folder-but-nothing-happens-in-unreal-project","text":"First make sure your project's .uproject file is referencing the plugin. Then make sure you have run clean.bat and then GenerateProjectFiles.bat as described in Overview above.","title":"I copied Unreal\\Plugins folder but nothing happens in Unreal Project."},{"location":"dev_workflow/#i-have-multiple-unreal-projects-with-airsim-plugin-how-do-i-update-them-easily","text":"You are in luck! We have build_all_ue_projects.bat which exactly does that. Don't treat it as black box (at least not yet), open it up and see what it does. It has 4 variables that are being set from command line args. If these args is not supplied they are set to default values in next set of statements. You might want to change default values for the paths. This batch file builds AirSim plugin, deploys it to all listed projects (see CALL statements later in the batch file), runs packaging for those projects and puts final binaries in specified folder - all in one step! This is what we use to create our own binary releases.","title":"I have multiple Unreal projects with AirSim plugin. How do I update them easily?"},{"location":"dev_workflow/#how-do-i-contribute-back-to-airsim","text":"Before making any changes make sure you have created your feature branch. After you test your code changes in Blocks environment, follow the usual steps to make contributions just like any other GitHub projects. If you are not familiar with Git Branch-Rebase-Merge workflow, please read this first .","title":"How do I contribute back to AirSim?"},{"location":"hardware/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics gestureBot Construction Guide The gestureBot is adapted from the Mini robot kit from Robotis, Inc and the MSRAbot project created by interns at Microsoft Research Asia. The information on this page will guide you through purchasing and printing parts, as well as step-by-step assembly of the gestureBot. Before you begin... If you have experience in 3D printing and electronic assembly, we hope that you find this project relatively simple and straight-forward. Once the parts are in-hand, the time required to complete the project won't be much more than it takes to print the 3D models. If you are not experienced in these areas, one of the first things you'll learn is that desktop 3D-printers are not a good source of instant gratification! Even the smallest of the models contained in this kit will take several hours to print, while some of the larger models take more than half a day. With those time requirements in mind, we broke the procedures down into work sessions that fit along with the 3D print operations. Alternatively, a class of 5-10 students taking on this project might be successful with serial use of a single 3D printer and meeting once-a-week to perform the other assembly tasks. An important feature of these instructions is their sequence. Following the steps in the sessions in order is key to preventing the need to disassemble an assembly in order to have access to cable pathways or screw and rivet holes. Safety Alert: We've found that a key to success in building this and other projects using 3D-printed plastic parts is to know your own strength . Human hands can be more than strong enough to damage or destroy plastic parts, especially when muscle force is amplified by the leverage provided by a tool. For example, gripping a screwdriver with your fingers instead of the palm of your hand will limit the torque force you can apply with your muscles when driving a metal screw into a plastic servo horn (wheel). When applying muscle force with a tool against a plastic part, being aware of where the stationary force from your hand, body, or workbench is pushing back against your tool can avoid suddenly breaking the part and possibly causing injury. It is also helpful to use vision as well as force when tightening a screw. Paying attention to the closing gaps when turning a screw can help avoid stripping threads in the plastic and will make for stronger connections. A famous book that discusses this concept of \"mechanic's feel\" as well as other challenges in life is Zen and the Art of Motorcycle Maintenance: An Inquiry into Values by Robert M. Persig. Requirements Here are the physical things you'll need to build and use your gestureBot: PC x64 CPU Microsoft Windows 10 USB3 port (USB2 can work in some cases) Tools 3D printer (we use an Ultimaker3 with PLA filament) Wire Stripper/Cutter small Phillips screwdriver (PH0) small flat-blade screwdriver (2mm) Alternative Screwdrivers: 5 in 1 Precision Screwdriver 16 in 1 Precision Screwdriver pen and light-colored tape for labeling servo ID's Optional but Helpful Tools Rivet Tool needle-nose pliers Magnet (for easier screw driving) 3mm drill bit, variable speed drill 3D-printing cleanup tools . A swivel deburring tool is especially useful. Parts List Purchased Parts ID Qty Name Purchase Link . P1 12 Robotis XL-320 Servo Motors (includes short connection cable) P2 1 USB3 Hub with Power Supply P3 1 Robotis OpenCM9.04c Servo Control Circuit Board P4 1 USB Terminal Block P5 1 Short USB Cable (USB-A to USB-Micro) P6 2 Long (19cm) Servo Connection Cables P7 1 6mm Plastic Rivets, (40) short (2-step) and (10) long (3-step) P7 (alternate) 1 6mm Plastic Rivets (weaker but lower-cost, variety of colors) P8 1 Screw Set 3D Printed Parts ID Qty Name Model Link . M1 1 Base M2 1 Base Frame M3 1 Bracket Cover (Rivet Side) M4 1 Bracket Cover (Screw Side) M5 2 Ear M6 1 Eyes M7 1 Face M8 2 Hand M9 1 Head Frame M10 1 Hip Cover M11 1 Hood M12 1 Neck M13 1 Servo Cover M14 2 Servo Mount Plate M15 2 Servo Side Cover M16 2 Servo Wheel M17 2 Speaker M18 2 Head Swing Bracket M19 2 Hip Swing Bracket M20 1 Screw-Mount Swing Bracket M21 1 Upper Torso Back Cover M22 1 Upper Torso Front Cover M23 1 Upper Torso Frame Step-by-Step Instructions The 3D-printed parts are used to organize this project's task steps into sessions. 3D printing a part usually requires more time than is reasonable for a work session, often leaving the print operation to complete overnight as the most practical solution. Session 1: Set Up the Servo Controller Electronics In the first session we will begin the process of 3D-printing the gestureBot's plastic parts starting with the base frame while connecting the servo controller to the powered USB3 Hub and a single servo motor. Session 2: Configure the Servos In the second session we will 3D-print the gestureBot's torso components while establishing the communication network for our servo motors by connecting them one-by-one to the PC and using using the Robotis Dynamixel Wizard 2.0 application to program each with a unique ID. Session 3: Assemble the Torso In the third session we will 3D-print the head components while assembling the gestureBot's torso. Session 4: Assemble the Head Structure In the fourth session we will 3D-print the components for both arms while assembling the structural components of the gestureBot's head. Session 5: Assemble the Right Arm In the fifth session we will 3D-print the base component while assembling the gestureBot's right arm. Session 6: Assemble the Left Arm In the sixth session we will 3D-print the eye and ear components (in an alternate color if desired) while assembling the gestureBot's left arm. Session 7: Attach the Arms and Test the Servos In the seventh session we will 3D-print the hood component (in an alternate color if desired) while we attach the arms to the torso and test the servos using the Robotis Dynamixel Wizard 2.0 application. Session 8: Complete the Head In the eigth session we will 3D-print the hands and arm servo covers while we assemble the the head components. Session 9: Install the Body Covers In the ninth session we will 3D-print the torso front and back covers while we install the arm servo covers and hands. Session 10: Run the Gesture Applications In the tenth session we will 3D-print the torso cover components while we attach the hands and the arm servo covers. At this point the gestureBot will be ready to run the Labanotation Suite software applications. The torso covers can be attached as soon as they are finished printing. A last word... Something to keep in mind is that this kit is designed to be changed and adapted. The 3D models are simple and lend themselves to modifications using such tools as Blender . Additional devices such as cameras, microphones, and loudspeakers can be added using the two available USB ports in the base of the robot, as well as leveraging other devices and connections through the host PC. We can't wait to see what you come up with! --the Microsoft Applied Robotics Research Team","title":"Home"},{"location":"hardware/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"hardware/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"hardware/#gesturebot-construction-guide","text":"The gestureBot is adapted from the Mini robot kit from Robotis, Inc and the MSRAbot project created by interns at Microsoft Research Asia. The information on this page will guide you through purchasing and printing parts, as well as step-by-step assembly of the gestureBot.","title":"gestureBot Construction Guide"},{"location":"hardware/#before-you-begin","text":"If you have experience in 3D printing and electronic assembly, we hope that you find this project relatively simple and straight-forward. Once the parts are in-hand, the time required to complete the project won't be much more than it takes to print the 3D models. If you are not experienced in these areas, one of the first things you'll learn is that desktop 3D-printers are not a good source of instant gratification! Even the smallest of the models contained in this kit will take several hours to print, while some of the larger models take more than half a day. With those time requirements in mind, we broke the procedures down into work sessions that fit along with the 3D print operations. Alternatively, a class of 5-10 students taking on this project might be successful with serial use of a single 3D printer and meeting once-a-week to perform the other assembly tasks. An important feature of these instructions is their sequence. Following the steps in the sessions in order is key to preventing the need to disassemble an assembly in order to have access to cable pathways or screw and rivet holes.","title":"Before you begin..."},{"location":"hardware/#safety-alert","text":"We've found that a key to success in building this and other projects using 3D-printed plastic parts is to know your own strength . Human hands can be more than strong enough to damage or destroy plastic parts, especially when muscle force is amplified by the leverage provided by a tool. For example, gripping a screwdriver with your fingers instead of the palm of your hand will limit the torque force you can apply with your muscles when driving a metal screw into a plastic servo horn (wheel). When applying muscle force with a tool against a plastic part, being aware of where the stationary force from your hand, body, or workbench is pushing back against your tool can avoid suddenly breaking the part and possibly causing injury. It is also helpful to use vision as well as force when tightening a screw. Paying attention to the closing gaps when turning a screw can help avoid stripping threads in the plastic and will make for stronger connections. A famous book that discusses this concept of \"mechanic's feel\" as well as other challenges in life is Zen and the Art of Motorcycle Maintenance: An Inquiry into Values by Robert M. Persig.","title":"Safety Alert:"},{"location":"hardware/#requirements","text":"Here are the physical things you'll need to build and use your gestureBot:","title":"Requirements"},{"location":"hardware/#pc","text":"x64 CPU Microsoft Windows 10 USB3 port (USB2 can work in some cases)","title":"PC"},{"location":"hardware/#tools","text":"3D printer (we use an Ultimaker3 with PLA filament) Wire Stripper/Cutter small Phillips screwdriver (PH0) small flat-blade screwdriver (2mm) Alternative Screwdrivers: 5 in 1 Precision Screwdriver 16 in 1 Precision Screwdriver pen and light-colored tape for labeling servo ID's","title":"Tools"},{"location":"hardware/#optional-but-helpful-tools","text":"Rivet Tool needle-nose pliers Magnet (for easier screw driving) 3mm drill bit, variable speed drill 3D-printing cleanup tools . A swivel deburring tool is especially useful.","title":"Optional but Helpful Tools"},{"location":"hardware/#parts-list","text":"","title":"Parts List"},{"location":"hardware/#purchased-parts","text":"ID Qty Name Purchase Link . P1 12 Robotis XL-320 Servo Motors (includes short connection cable) P2 1 USB3 Hub with Power Supply P3 1 Robotis OpenCM9.04c Servo Control Circuit Board P4 1 USB Terminal Block P5 1 Short USB Cable (USB-A to USB-Micro) P6 2 Long (19cm) Servo Connection Cables P7 1 6mm Plastic Rivets, (40) short (2-step) and (10) long (3-step) P7 (alternate) 1 6mm Plastic Rivets (weaker but lower-cost, variety of colors) P8 1 Screw Set","title":"Purchased Parts"},{"location":"hardware/#3d-printed-parts","text":"ID Qty Name Model Link . M1 1 Base M2 1 Base Frame M3 1 Bracket Cover (Rivet Side) M4 1 Bracket Cover (Screw Side) M5 2 Ear M6 1 Eyes M7 1 Face M8 2 Hand M9 1 Head Frame M10 1 Hip Cover M11 1 Hood M12 1 Neck M13 1 Servo Cover M14 2 Servo Mount Plate M15 2 Servo Side Cover M16 2 Servo Wheel M17 2 Speaker M18 2 Head Swing Bracket M19 2 Hip Swing Bracket M20 1 Screw-Mount Swing Bracket M21 1 Upper Torso Back Cover M22 1 Upper Torso Front Cover M23 1 Upper Torso Frame","title":"3D Printed Parts"},{"location":"hardware/#step-by-step-instructions","text":"The 3D-printed parts are used to organize this project's task steps into sessions. 3D printing a part usually requires more time than is reasonable for a work session, often leaving the print operation to complete overnight as the most practical solution.","title":"Step-by-Step Instructions"},{"location":"hardware/#session-1-set-up-the-servo-controller-electronics","text":"In the first session we will begin the process of 3D-printing the gestureBot's plastic parts starting with the base frame while connecting the servo controller to the powered USB3 Hub and a single servo motor.","title":"Session 1: Set Up the Servo Controller Electronics"},{"location":"hardware/#session-2-configure-the-servos","text":"In the second session we will 3D-print the gestureBot's torso components while establishing the communication network for our servo motors by connecting them one-by-one to the PC and using using the Robotis Dynamixel Wizard 2.0 application to program each with a unique ID.","title":"Session 2: Configure the Servos"},{"location":"hardware/#session-3-assemble-the-torso","text":"In the third session we will 3D-print the head components while assembling the gestureBot's torso.","title":"Session 3: Assemble the Torso"},{"location":"hardware/#session-4-assemble-the-head-structure","text":"In the fourth session we will 3D-print the components for both arms while assembling the structural components of the gestureBot's head.","title":"Session 4: Assemble the Head Structure"},{"location":"hardware/#session-5-assemble-the-right-arm","text":"In the fifth session we will 3D-print the base component while assembling the gestureBot's right arm.","title":"Session 5: Assemble the Right Arm"},{"location":"hardware/#session-6-assemble-the-left-arm","text":"In the sixth session we will 3D-print the eye and ear components (in an alternate color if desired) while assembling the gestureBot's left arm.","title":"Session 6: Assemble the Left Arm"},{"location":"hardware/#session-7-attach-the-arms-and-test-the-servos","text":"In the seventh session we will 3D-print the hood component (in an alternate color if desired) while we attach the arms to the torso and test the servos using the Robotis Dynamixel Wizard 2.0 application.","title":"Session 7: Attach the Arms and Test the Servos"},{"location":"hardware/#session-8-complete-the-head","text":"In the eigth session we will 3D-print the hands and arm servo covers while we assemble the the head components.","title":"Session 8: Complete the Head"},{"location":"hardware/#session-9-install-the-body-covers","text":"In the ninth session we will 3D-print the torso front and back covers while we install the arm servo covers and hands.","title":"Session 9: Install the Body Covers"},{"location":"hardware/#session-10-run-the-gesture-applications","text":"In the tenth session we will 3D-print the torso cover components while we attach the hands and the arm servo covers. At this point the gestureBot will be ready to run the Labanotation Suite software applications. The torso covers can be attached as soon as they are finished printing.","title":"Session 10: Run the Gesture Applications"},{"location":"hardware/#a-last-word","text":"Something to keep in mind is that this kit is designed to be changed and adapted. The 3D models are simple and lend themselves to modifications using such tools as Blender . Additional devices such as cameras, microphones, and loudspeakers can be added using the two available USB ports in the base of the robot, as well as leveraging other devices and connections through the host PC. We can't wait to see what you come up with! --the Microsoft Applied Robotics Research Team","title":"A last word..."},{"location":"hardware/Session01/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics gestureBot Construction Guide Session 1: Set Up the Servo Controller Electronics In this first session we will begin the process of 3D-printing robot plastic parts starting with the base frame which will be used in Session 2. Then, we will connect the servo controller to the powered USB3 Hub and a single servo motor. Parts: (2) servos and their included 130mm cables USB Terminal Block USB3 Hub with Power Supply Servo Controller Tools: wire strippers/cutters 2mm screwdriver Procedure: First, start 3D-printing the parts required for the next session: Base Frame Second, connect the controller and Terminal Block to the USB Hub: From two servo packages, remove the short connection cables and servos then set one servo aside. Using the wire strippers/cutters, cut the plastic connector off of one of the short connection cables. Cut the Pin3 wire completely off the remaining connector and save it to use as a ground jumper wire. Tip: Double check you are cutting the correct wire! Using the wire stripper's stranded 22-gauge slot, remove approximately 3mm of the insulation from each end of the ground jumper wire and the remaining two ends of the connector cable. Using your fingers, twist the wire ends so that the strands hold together. Using the 2mm screwdriver, loosen the screw-clamps on the USB terminal block marked ' S ', ' - ', and ' + '. Insert one end of the prepared jumper wire into the USB terminal block's screw clamp marked \" - \" then tighten with the screw driver. Tip: Make sure the wire is correctly connected by firmly pulling on it and visually checking that it is not slipping out Connect the wires from the short cable to the USB Terminal Block: Insert both the loose end of the prepared wire clamped into ' - ' and the cable wire connected to pin 1 into the wire clamp marked ' S ' then tighten with the screwdriver. Insert the cable wire connected to pin 2 on the connector into the wire clamp marked \" + \" (for positive voltage) and tighten with the screwdriver. Tip: the jumper wire ties the negative side of the power provided by USB to chassis ground which reduces electromagnetic noise that may interfere with communications between the controller and the servos. Connect the USB Terminal Block to the Servo Controller with any one of the four three-pin connectors on the controller. Plug the USB Terminal Block into the USB Hub using the port closest to the hub's link cable. Using the short USB-to-mini cable, connect the servo controller to any of the USB Hub's ports. Connect the servo to the servo controller with the remaining 130mm cable. The cable connector must snap into place on either connector on the servo. The other end of the cable can be inserted into any of the three remaining 3-pin connectors on the servo controller PC board. Tip: Look closely at the cable connector ends and the receptacles to make sure the pin orientation is correct. If you push hard enough, it is possible to mistakenly insert them backwards. When inserted correctly, they will \"snap\" into place. Third, power it up! Plug the USB Hub's power supply into an AC receptacle and insert its connector into the USB Hub. Observe a LED inside the servo flash red momentarily and a green LED on the controller turn on when power is applied. Plug the USB Hub into your PC, then use Windows to open Device Manager and note which serial port number the servo controller is assigned to (for example: \"COM4\" ) Next - Session 2 - Communicate with the Servos","title":"Session01"},{"location":"hardware/Session01/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"hardware/Session01/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"hardware/Session01/#gesturebot-construction-guide","text":"","title":"gestureBot Construction Guide"},{"location":"hardware/Session01/#session-1-set-up-the-servo-controller-electronics","text":"In this first session we will begin the process of 3D-printing robot plastic parts starting with the base frame which will be used in Session 2. Then, we will connect the servo controller to the powered USB3 Hub and a single servo motor.","title":"Session 1: Set Up the Servo Controller Electronics"},{"location":"hardware/Session01/#parts","text":"(2) servos and their included 130mm cables USB Terminal Block USB3 Hub with Power Supply Servo Controller","title":"Parts:"},{"location":"hardware/Session01/#tools","text":"wire strippers/cutters 2mm screwdriver","title":"Tools:"},{"location":"hardware/Session01/#procedure","text":"","title":"Procedure:"},{"location":"hardware/Session01/#first-start-3d-printing-the-parts-required-for-the-next-session","text":"Base Frame","title":"First, start 3D-printing the parts required for the next session:"},{"location":"hardware/Session01/#second-connect-the-controller-and-terminal-block-to-the-usb-hub","text":"From two servo packages, remove the short connection cables and servos then set one servo aside. Using the wire strippers/cutters, cut the plastic connector off of one of the short connection cables. Cut the Pin3 wire completely off the remaining connector and save it to use as a ground jumper wire. Tip: Double check you are cutting the correct wire! Using the wire stripper's stranded 22-gauge slot, remove approximately 3mm of the insulation from each end of the ground jumper wire and the remaining two ends of the connector cable. Using your fingers, twist the wire ends so that the strands hold together. Using the 2mm screwdriver, loosen the screw-clamps on the USB terminal block marked ' S ', ' - ', and ' + '. Insert one end of the prepared jumper wire into the USB terminal block's screw clamp marked \" - \" then tighten with the screw driver. Tip: Make sure the wire is correctly connected by firmly pulling on it and visually checking that it is not slipping out Connect the wires from the short cable to the USB Terminal Block: Insert both the loose end of the prepared wire clamped into ' - ' and the cable wire connected to pin 1 into the wire clamp marked ' S ' then tighten with the screwdriver. Insert the cable wire connected to pin 2 on the connector into the wire clamp marked \" + \" (for positive voltage) and tighten with the screwdriver. Tip: the jumper wire ties the negative side of the power provided by USB to chassis ground which reduces electromagnetic noise that may interfere with communications between the controller and the servos. Connect the USB Terminal Block to the Servo Controller with any one of the four three-pin connectors on the controller. Plug the USB Terminal Block into the USB Hub using the port closest to the hub's link cable. Using the short USB-to-mini cable, connect the servo controller to any of the USB Hub's ports. Connect the servo to the servo controller with the remaining 130mm cable. The cable connector must snap into place on either connector on the servo. The other end of the cable can be inserted into any of the three remaining 3-pin connectors on the servo controller PC board. Tip: Look closely at the cable connector ends and the receptacles to make sure the pin orientation is correct. If you push hard enough, it is possible to mistakenly insert them backwards. When inserted correctly, they will \"snap\" into place.","title":"Second, connect the controller and Terminal Block to the USB Hub:"},{"location":"hardware/Session01/#third-power-it-up","text":"Plug the USB Hub's power supply into an AC receptacle and insert its connector into the USB Hub. Observe a LED inside the servo flash red momentarily and a green LED on the controller turn on when power is applied. Plug the USB Hub into your PC, then use Windows to open Device Manager and note which serial port number the servo controller is assigned to (for example: \"COM4\" )","title":"Third, power it up!"},{"location":"hardware/Session01/#next-session-2-communicate-with-the-servos","text":"","title":"Next -&gt; Session 2 - Communicate with the Servos"},{"location":"hardware/Session02/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics gestureBot Construction Guide Session 2: Configure the Servos In this session we will 3D-print the robot's torso components while establishing the communication network for our servo motors by connecting them one-by-one to the PC and using a software tool to program each with a unique ID. Parts: USB hub and power supply, servo, and servo controller assembly from Session 1 servo set aside in Session 1 (10) servos and their 130mm cables Tools: pen and light-colored tape for labeling servo ID's Procedure: First, start 3D-printing the parts required for the next session: Hip Cover Hip Swing Bracket Servo Wheel Upper Torso Frame Servo Wheel Servo Mount Plate Tip: Many 3D-printers have enough room on their build plate to print all of these models at the same time. Follow your printer's software instructions on how to add multiple models to a printing project file. Second, download and install the servo configuration software: Download the Robotis Dynamixel Wizard 2.0 software appropriate for you platform from this web page: http://en.robotis.com/service/downloadpage.php?ca_id=10 Install and launch the Dynamixel Wizard 2.0 application, which provides a Graphical User Interface (GUI) for configuring the servo motors. If needed, installation instructions are provided on the this web page: https://emanual.robotis.com/docs/en/software/dynamixel/dynamixel_wizard2/#introduction Select the Options button on the top level menu In the Scan section, adjust the following settings: select only Protocol 2.0 select the com port that showed up in Device Manager select only 1000000 bps leave the other options at default select OK Select the Scan button on the top level menu. The scan should find one XL-320 servo at ID:001 in the list on the left side of the GUI,and show a list of the servo's memory page in the center of the GUI. Also, a device listed as Unknown at ID:200 will appear in the list. This represents the servo controller PC board and can be ignored. In the left-side list, select the [ID:001]XL-320 servo node. In the memory page list, select and highlight row Address 3, which contains the servo's ID. If the servo is brand new out-of-the-package, it should already be set to Decimal value \"1\". When the Address row is selected, a window labeled ID will appear in the lower right corner of the application GUI. In the list of available addresses, select address 2 (Decimal) and then select \"Save\" below. The memory page list should now show the value Decimal 2 in the ID row. In the upper right corner of the UI basic controls for the XL-320 are available. By turning the Torque switch on and then selecting a position on the wheel, the servo horn will rotate to that position. Feel free to try out turning the LED off and on and switching from the Joint to Wheel mode. You will notice that other UI's appear on the right side of the application when different rows are selected in the memory page. For now, don't change any other parameter other than the servo ID. Tip: Be sure to return to Joint mode before you finish. Label the newly programmed servo as ID:2 with tape and a pen, or return the servo to the bag and label the bag, or whatever method works best for you to identify the servo ID later. Third, program the servo ID's from 1 to 12: Disconnect the labeled servo and repeat the process above until each of the servos has been programmed and labeled with a unique ID from 1 through 12. Tip: Removing the cable from the servo requires significant force, but the Robotis cables are strong and designed for this. Grip the cable between your thumb and the side of your index finger near the servo-side connector and grip the servo with your other hand. Without jerking, pull the cable firmly up and out of the connector. Next - Session 3 : Assemble the Torso","title":"Session02"},{"location":"hardware/Session02/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"hardware/Session02/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"hardware/Session02/#gesturebot-construction-guide","text":"","title":"gestureBot Construction Guide"},{"location":"hardware/Session02/#session-2-configure-the-servos","text":"In this session we will 3D-print the robot's torso components while establishing the communication network for our servo motors by connecting them one-by-one to the PC and using a software tool to program each with a unique ID.","title":"Session 2: Configure the Servos"},{"location":"hardware/Session02/#parts","text":"USB hub and power supply, servo, and servo controller assembly from Session 1 servo set aside in Session 1 (10) servos and their 130mm cables","title":"Parts:"},{"location":"hardware/Session02/#tools","text":"pen and light-colored tape for labeling servo ID's","title":"Tools:"},{"location":"hardware/Session02/#procedure","text":"","title":"Procedure:"},{"location":"hardware/Session02/#first-start-3d-printing-the-parts-required-for-the-next-session","text":"Hip Cover Hip Swing Bracket Servo Wheel Upper Torso Frame Servo Wheel Servo Mount Plate Tip: Many 3D-printers have enough room on their build plate to print all of these models at the same time. Follow your printer's software instructions on how to add multiple models to a printing project file.","title":"First, start 3D-printing the parts required for the next session:"},{"location":"hardware/Session02/#second-download-and-install-the-servo-configuration-software","text":"Download the Robotis Dynamixel Wizard 2.0 software appropriate for you platform from this web page: http://en.robotis.com/service/downloadpage.php?ca_id=10 Install and launch the Dynamixel Wizard 2.0 application, which provides a Graphical User Interface (GUI) for configuring the servo motors. If needed, installation instructions are provided on the this web page: https://emanual.robotis.com/docs/en/software/dynamixel/dynamixel_wizard2/#introduction Select the Options button on the top level menu In the Scan section, adjust the following settings: select only Protocol 2.0 select the com port that showed up in Device Manager select only 1000000 bps leave the other options at default select OK Select the Scan button on the top level menu. The scan should find one XL-320 servo at ID:001 in the list on the left side of the GUI,and show a list of the servo's memory page in the center of the GUI. Also, a device listed as Unknown at ID:200 will appear in the list. This represents the servo controller PC board and can be ignored. In the left-side list, select the [ID:001]XL-320 servo node. In the memory page list, select and highlight row Address 3, which contains the servo's ID. If the servo is brand new out-of-the-package, it should already be set to Decimal value \"1\". When the Address row is selected, a window labeled ID will appear in the lower right corner of the application GUI. In the list of available addresses, select address 2 (Decimal) and then select \"Save\" below. The memory page list should now show the value Decimal 2 in the ID row. In the upper right corner of the UI basic controls for the XL-320 are available. By turning the Torque switch on and then selecting a position on the wheel, the servo horn will rotate to that position. Feel free to try out turning the LED off and on and switching from the Joint to Wheel mode. You will notice that other UI's appear on the right side of the application when different rows are selected in the memory page. For now, don't change any other parameter other than the servo ID. Tip: Be sure to return to Joint mode before you finish. Label the newly programmed servo as ID:2 with tape and a pen, or return the servo to the bag and label the bag, or whatever method works best for you to identify the servo ID later.","title":"Second, download and install the servo configuration software:"},{"location":"hardware/Session02/#third-program-the-servo-ids-from-1-to-12","text":"Disconnect the labeled servo and repeat the process above until each of the servos has been programmed and labeled with a unique ID from 1 through 12. Tip: Removing the cable from the servo requires significant force, but the Robotis cables are strong and designed for this. Grip the cable between your thumb and the side of your index finger near the servo-side connector and grip the servo with your other hand. Without jerking, pull the cable firmly up and out of the connector.","title":"Third, program the servo ID's from 1 to 12:"},{"location":"hardware/Session02/#next-session-3-assemble-the-torso","text":"","title":"Next -&gt; Session 3: Assemble the Torso"},{"location":"hardware/Session03/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics gestureBot Construction Guide Session 3: Assemble the Torso In this session we will 3D-print the head components while assembling the gestureBot's torso. Parts: (3) servos labeled ID:004, ID:003, and ID:002 programmed in Session 2 (4) 130MM servo cables (22) short plastic rivets (5) long plastic rivets (4) 3mm miniature steel screws Base Frame 3D-printed in Session 1, the base frame is the lower torso component and provides the stationary mounting point for servo ID:004 and anchors all of the moving parts of the gestureBot. Hip Cover 3D-printed in Session 2, the hip cover attaches to the base frame and it's spherical shape allows the gestureBot's hip joint to move without exposing its interior. Hip Swing Bracket 3D-printed in Session 2, the hip swing bracket and servo wheel attach to servo ID:004 and ID:003 rotating vertically to allow the entire gestureBot to lean forward and backward. Upper Torso Frame 3D-printed in Session 2, the upper torso frame and mount plate connect to servo ID:003 allowing the gestureBot's torso to rotate horizontally, as well as providing mount points for servo ID:002 which rotates the head horizontally and servos ID:005 and ID:009 which rotate the arm assemblies vertically. Servo Wheel 3D-printed in Session 2, the servo wheel supports servo ID:004 in the hip swing bracket. Servo Mount Plate 3D-printed in Session 2, the servo mount plate supports servo ID:002 in the upper torso frame. Tools: PH0 Phillips screwdriver plastic rivet tool Procedure: First, start 3D-printing the parts required for the next session: Neck 3D-printed in Session 3 Head Swing Bracket 3D-printed in Session 3 Servo Wheel 3D-printed in Session 3 Head Frame 3D-printed in Session 3 Face 3D-printed in Session 3 Speaker 3D-printed in Session 3 Hint: Many 3D-printers have enough room on their build plate to print all of these models at the same time. Follow your printer's software instructions on how to add multiple models to a printing project file. Second, learn how to ensure the gripping performance of plastic rivets: It is easy to install plastic rivets incorrectly and decrease their grip and fastening strength. To work well, care must be taken not to deform their shape or crush their structure when inserting them into their mount holes. Generally, the plastic covers of the servo motors are molded (not 3D-printed) from high-quality plastic. Their precision shapes are stable and do not present a problem for plastic rivets. However, parts printed by desktop 3D-printers may have shape variances that may negatively impact the way the rivets work. Issues such as printer settings, calibration drift, quality and type of the plastic filament supply, print head wear, and environmental conditions such as temperature, humidity or vibrations can all affect the strength, flexibility, and shape precision of the parts they print. To prevent these factors from impacting rivet strength, make sure that the mount holes are a full 3mm in diameter. If they are too small they will crush and deform the rivet shell, preventing their tips from expanding when the pin is inserted and gripping the inside edge of the mount hole. If this is the case with the mount holes in your 3D-printed parts, be sure to enlarge them with a 3mm drill bit, a rotary deburring tool, or a needle file. When inserting a rivet, the shell must reach far enough into the material layers it is fastening that the ridges on its end extend past the edge of the last layer. Ideally, the ridge edge and the material edge match perfectly. When the pin is inserted, the shell expands and pushes the ridge edges over the material edge and should produce a satisfying \"snap\" feeling. This action creates a gripping shelf that prevents the rivet from exiting the hole. If the rivet does not fully expand, only the friction pressure of the rivet shell against the side of the hole will hold it in, which is a much weaker form of fastening and generally will work loose as the robot moves during normal operation. Third, assemble the components of the torso: Mount servo ID:003 to the hip bracket by installing 6 rivets. Connect (2) 130mm cables to servo. Install servo wheel with a long rivet. Do not insert a pin into the long rivet, friction will be sufficient to hold the rivet through the 3 layers and the servo wheel must spin freely. Install servo ID:004 into the hip bracket, making sure that the servo horn (wheel that moves) is at its 0-degree postion. Check that the small single tick-mark on the horn is aligned with the tick-mark on the servo housing, as well as the tick-mark on the swing bracket. Fasten servo ID:004 to the bracket with (4) screws. Connect servo ID003 to servo ID004 with one of its 130mm cables. Connect 130mm cable to servo ID004. Mount servo ID:004 to base frame with (6) rivets, routing cable through side gap between mount holes. Route servo ID:004 cable through access hole in base frame. Install the hip cover over servos ID:003 and ID:004 and onto the base frame. While not functionally significant, it might be aesthetically preferrable to align the ridges of the hip cover with the ridges on the base frame. Mount upper torso frame to servo ID003 with (4) short rivets, taking care servo horn tick-mark is aligned with tick-mark on servo housing. Attach servo ID:002 mount plate with (4) short rivets. Connect servo ID:003 to servo ID:002 with 130mm cable Mount servo ID:002 to upper torso frame with mount plate, taking care to route 130mm cables through gaps between mounting holes at bottom of servo. Use (2) long rivets in the bottom mount plate holes. Use (4) short rivets in the back mount plate holes. Inspect The completed upper torso assembly. Next - Session 4: Assemble the Head Structure","title":"Session03"},{"location":"hardware/Session03/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"hardware/Session03/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"hardware/Session03/#gesturebot-construction-guide","text":"","title":"gestureBot Construction Guide"},{"location":"hardware/Session03/#session-3-assemble-the-torso","text":"In this session we will 3D-print the head components while assembling the gestureBot's torso.","title":"Session 3: Assemble the Torso"},{"location":"hardware/Session03/#parts","text":"(3) servos labeled ID:004, ID:003, and ID:002 programmed in Session 2 (4) 130MM servo cables (22) short plastic rivets (5) long plastic rivets (4) 3mm miniature steel screws Base Frame 3D-printed in Session 1, the base frame is the lower torso component and provides the stationary mounting point for servo ID:004 and anchors all of the moving parts of the gestureBot. Hip Cover 3D-printed in Session 2, the hip cover attaches to the base frame and it's spherical shape allows the gestureBot's hip joint to move without exposing its interior. Hip Swing Bracket 3D-printed in Session 2, the hip swing bracket and servo wheel attach to servo ID:004 and ID:003 rotating vertically to allow the entire gestureBot to lean forward and backward. Upper Torso Frame 3D-printed in Session 2, the upper torso frame and mount plate connect to servo ID:003 allowing the gestureBot's torso to rotate horizontally, as well as providing mount points for servo ID:002 which rotates the head horizontally and servos ID:005 and ID:009 which rotate the arm assemblies vertically. Servo Wheel 3D-printed in Session 2, the servo wheel supports servo ID:004 in the hip swing bracket. Servo Mount Plate 3D-printed in Session 2, the servo mount plate supports servo ID:002 in the upper torso frame.","title":"Parts:"},{"location":"hardware/Session03/#tools","text":"PH0 Phillips screwdriver plastic rivet tool","title":"Tools:"},{"location":"hardware/Session03/#procedure","text":"","title":"Procedure:"},{"location":"hardware/Session03/#first-start-3d-printing-the-parts-required-for-the-next-session","text":"Neck 3D-printed in Session 3 Head Swing Bracket 3D-printed in Session 3 Servo Wheel 3D-printed in Session 3 Head Frame 3D-printed in Session 3 Face 3D-printed in Session 3 Speaker 3D-printed in Session 3 Hint: Many 3D-printers have enough room on their build plate to print all of these models at the same time. Follow your printer's software instructions on how to add multiple models to a printing project file.","title":"First, start 3D-printing the parts required for the next session:"},{"location":"hardware/Session03/#second-learn-how-to-ensure-the-gripping-performance-of-plastic-rivets","text":"It is easy to install plastic rivets incorrectly and decrease their grip and fastening strength. To work well, care must be taken not to deform their shape or crush their structure when inserting them into their mount holes. Generally, the plastic covers of the servo motors are molded (not 3D-printed) from high-quality plastic. Their precision shapes are stable and do not present a problem for plastic rivets. However, parts printed by desktop 3D-printers may have shape variances that may negatively impact the way the rivets work. Issues such as printer settings, calibration drift, quality and type of the plastic filament supply, print head wear, and environmental conditions such as temperature, humidity or vibrations can all affect the strength, flexibility, and shape precision of the parts they print. To prevent these factors from impacting rivet strength, make sure that the mount holes are a full 3mm in diameter. If they are too small they will crush and deform the rivet shell, preventing their tips from expanding when the pin is inserted and gripping the inside edge of the mount hole. If this is the case with the mount holes in your 3D-printed parts, be sure to enlarge them with a 3mm drill bit, a rotary deburring tool, or a needle file. When inserting a rivet, the shell must reach far enough into the material layers it is fastening that the ridges on its end extend past the edge of the last layer. Ideally, the ridge edge and the material edge match perfectly. When the pin is inserted, the shell expands and pushes the ridge edges over the material edge and should produce a satisfying \"snap\" feeling. This action creates a gripping shelf that prevents the rivet from exiting the hole. If the rivet does not fully expand, only the friction pressure of the rivet shell against the side of the hole will hold it in, which is a much weaker form of fastening and generally will work loose as the robot moves during normal operation.","title":"Second, learn how to ensure the gripping performance of plastic rivets:"},{"location":"hardware/Session03/#third-assemble-the-components-of-the-torso","text":"Mount servo ID:003 to the hip bracket by installing 6 rivets. Connect (2) 130mm cables to servo. Install servo wheel with a long rivet. Do not insert a pin into the long rivet, friction will be sufficient to hold the rivet through the 3 layers and the servo wheel must spin freely. Install servo ID:004 into the hip bracket, making sure that the servo horn (wheel that moves) is at its 0-degree postion. Check that the small single tick-mark on the horn is aligned with the tick-mark on the servo housing, as well as the tick-mark on the swing bracket. Fasten servo ID:004 to the bracket with (4) screws. Connect servo ID003 to servo ID004 with one of its 130mm cables. Connect 130mm cable to servo ID004. Mount servo ID:004 to base frame with (6) rivets, routing cable through side gap between mount holes. Route servo ID:004 cable through access hole in base frame. Install the hip cover over servos ID:003 and ID:004 and onto the base frame. While not functionally significant, it might be aesthetically preferrable to align the ridges of the hip cover with the ridges on the base frame. Mount upper torso frame to servo ID003 with (4) short rivets, taking care servo horn tick-mark is aligned with tick-mark on servo housing. Attach servo ID:002 mount plate with (4) short rivets. Connect servo ID:003 to servo ID:002 with 130mm cable Mount servo ID:002 to upper torso frame with mount plate, taking care to route 130mm cables through gaps between mounting holes at bottom of servo. Use (2) long rivets in the bottom mount plate holes. Use (4) short rivets in the back mount plate holes. Inspect The completed upper torso assembly.","title":"Third, assemble the components of the torso:"},{"location":"hardware/Session03/#next-session-4-assemble-the-head-structure","text":"","title":"Next -&gt; Session 4: Assemble the Head Structure"},{"location":"hardware/Session04/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics gestureBot Construction Guide Session 4: Assemble the Head Structure In this session we will 3D-print the components for both arms while assembling the structural components of the gestureBot's head. Parts: servo labeled ID:001 programmed in Session 2 130MM servo cable (22) short plastic rivets (5) long plastic rivets (8) 3mm miniature steel screws Neck 3D-printed in Session 3, the neck is connected to servo ID:002 that rotates horizontally allowing the gestureBot to turn its head and servo ID:001. Head Swing Bracket 3D-printed in Session 3, the head swing bracket and servo wheel are attached to servo ID:001 that rotates vertically allowing the gestureBot to look up and down. Servo Wheel 3D-printed in Session 3, the servo wheel supports servo ID:001 in the head swing bracket. Head Frame 3D-printed in Session 3, the head frame attaches to the head swing bracket and provides a mount point for the speaker and face components. Face 3D-printed in Session 3, the face connects to the head frame and provides a mount point for the speaker and the eyes. Speaker 3D-printed in Session 3, the speaker is a mounting component intended as a placeholder for a speakspeaker modification in the future. Tools: PH0 Phillips screwdriver plastic rivet tool Procedure: First, start 3D-printing the parts required for both arm assemblies: (4) Screw Mount Swing Bracket (4) Servo Wheel (6) Servo Mount Plate Second, assemble the head structure: Mount the neck to the horn () of servo ID:002, taking care to align the tick-marks of the servo cover, the horn, and the neck component. Route the 130mm cable from servo ID:002 through the holes in the neck. Connect the 130mm cable to servo ID:001. Mount servo ID:001 to neck with (4) rivets. Mount the speaker component and head frame to face with (4) M1 10mm machine screws. Attach servo wheel to head swing bracket with long rivet. Do not install the rivet pin to allow the servo wheel to rotate freely. Install servo ID:001 to head swing bracket taking care to align servo horn tick-mark with tick-mark on bracket. Mount face frame to head swing bracket with (4) long rivets. Installing the rivet pins is not recommended to accomodate future modifications and disassembly. Inspect the gestureBot torso and head structure assemblies. Next- Session 5: Assemble the Right Arm","title":"Session04"},{"location":"hardware/Session04/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"hardware/Session04/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"hardware/Session04/#gesturebot-construction-guide","text":"","title":"gestureBot Construction Guide"},{"location":"hardware/Session04/#session-4-assemble-the-head-structure","text":"In this session we will 3D-print the components for both arms while assembling the structural components of the gestureBot's head.","title":"Session 4: Assemble the Head Structure"},{"location":"hardware/Session04/#parts","text":"servo labeled ID:001 programmed in Session 2 130MM servo cable (22) short plastic rivets (5) long plastic rivets (8) 3mm miniature steel screws Neck 3D-printed in Session 3, the neck is connected to servo ID:002 that rotates horizontally allowing the gestureBot to turn its head and servo ID:001. Head Swing Bracket 3D-printed in Session 3, the head swing bracket and servo wheel are attached to servo ID:001 that rotates vertically allowing the gestureBot to look up and down. Servo Wheel 3D-printed in Session 3, the servo wheel supports servo ID:001 in the head swing bracket. Head Frame 3D-printed in Session 3, the head frame attaches to the head swing bracket and provides a mount point for the speaker and face components. Face 3D-printed in Session 3, the face connects to the head frame and provides a mount point for the speaker and the eyes. Speaker 3D-printed in Session 3, the speaker is a mounting component intended as a placeholder for a speakspeaker modification in the future.","title":"Parts:"},{"location":"hardware/Session04/#tools","text":"PH0 Phillips screwdriver plastic rivet tool","title":"Tools:"},{"location":"hardware/Session04/#procedure","text":"","title":"Procedure:"},{"location":"hardware/Session04/#first-start-3d-printing-the-parts-required-for-both-arm-assemblies","text":"(4) Screw Mount Swing Bracket (4) Servo Wheel (6) Servo Mount Plate","title":"First, start 3D-printing the parts required for both arm assemblies:"},{"location":"hardware/Session04/#second-assemble-the-head-structure","text":"Mount the neck to the horn () of servo ID:002, taking care to align the tick-marks of the servo cover, the horn, and the neck component. Route the 130mm cable from servo ID:002 through the holes in the neck. Connect the 130mm cable to servo ID:001. Mount servo ID:001 to neck with (4) rivets. Mount the speaker component and head frame to face with (4) M1 10mm machine screws. Attach servo wheel to head swing bracket with long rivet. Do not install the rivet pin to allow the servo wheel to rotate freely. Install servo ID:001 to head swing bracket taking care to align servo horn tick-mark with tick-mark on bracket. Mount face frame to head swing bracket with (4) long rivets. Installing the rivet pins is not recommended to accomodate future modifications and disassembly. Inspect the gestureBot torso and head structure assemblies.","title":"Second, assemble the head structure:"},{"location":"hardware/Session04/#next-session-5-assemble-the-right-arm","text":"","title":"Next-&gt; Session 5: Assemble the Right Arm"},{"location":"hardware/Session05/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics gestureBot Construction Guide Session 5: Assemble the Right Arm In this session we will 3D-print the base component while assembling the gestureBot's right arm. Parts: servo labeled ID:005 programmed in Session 2 (3) 130MM servo cable (16) short plastic rivets (2) long plastic rivets (16) 3mm miniature steel screws (2) Screw Mount Swing Bracket 3D-printed in Session 4, the swing brackets and servo wheels form a shoulder joint between servos ID:005 and ID:006 and an elbow joint between servos ID:007 and ID:008. (2) Servo Wheel 3D-printed in Session 4, the servo wheels support the swing bracket mounts for servo ID:006 and ID:008. (2) Servo Mount Plate 3D-printed in Session 4, the servo mount plates support the servo ID:005 attachment to the upper torso frame and form a lower arm section between servo ID:006 and ID:007. Tools: PH0 Phillips screwdriver plastic rivet tool Procedure: First, start 3D-printing the parts required for future sessions: Base Second, assemble the right arm: Mount a swing bracket to servo ID:005 with (4) 3mm miniature screws, taking care to align the tick-marks on the servo horn, the servo cover, and the swing bracket. Attach servo wheel to right shoulder swing bracket by inserting a long rivet through the bracket and into the back of servo ID:006. Do not install the rivet pin to allow the servo wheel to rotate freely. Mount the servo ID:006 horn to the right shoulder swing bracket with (4) 3mm miniature screws and taking care to align servo horn double tick-marks with the tick-mark on the bracket. Note that in this case the alignment is made with the opposite side of the servo horn. With (3) short rivets, attach a servo mount plate to the back of servo ID:007 with the mount hole rings oriented length-wise to the servo. Connect a 130mm cable to the top-side connector of servo ID:006. Attach servo ID:006 to ID:007 using (4) short rivets in the mount plate while routing the cable connected to servo ID:006 through the space between the servo and the mount plate. Connect cable from servo ID:006 to servo ID:007 on its bottom-side connector. Mount the right elbow swing bracket to servo ID:007 with (4) 3mm miniature screws taking care to align the tick-mark on the servo horn with the tick-mark on the servo cover and the bracket. Attach servo wheel to right elbow swing bracket by inserting a long rivet through the bracket and into the back of servo ID:008. Do not install the rivet pin to allow the servo wheel to rotate freely. Mount the servo ID:008 horn to the right elbow swing bracket with (4) 3mm miniature screws and taking care to align servo horn double tick-marks with the tick-mark on the bracket. Note that in this case the alignment is made with the opposite side of the servo horn. Connect a 130mm cable from the top connector of servo ID:007 to the rear-side connector of servo ID:008. Connect a 130mm cable from the rear-side connector of servo ID:005 to the bottom-side connector of servo ID:006. Next- Session 6: Assemble the Left Arm","title":"Session05"},{"location":"hardware/Session05/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"hardware/Session05/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"hardware/Session05/#gesturebot-construction-guide","text":"","title":"gestureBot Construction Guide"},{"location":"hardware/Session05/#session-5-assemble-the-right-arm","text":"In this session we will 3D-print the base component while assembling the gestureBot's right arm.","title":"Session 5: Assemble the Right Arm"},{"location":"hardware/Session05/#parts","text":"servo labeled ID:005 programmed in Session 2 (3) 130MM servo cable (16) short plastic rivets (2) long plastic rivets (16) 3mm miniature steel screws (2) Screw Mount Swing Bracket 3D-printed in Session 4, the swing brackets and servo wheels form a shoulder joint between servos ID:005 and ID:006 and an elbow joint between servos ID:007 and ID:008. (2) Servo Wheel 3D-printed in Session 4, the servo wheels support the swing bracket mounts for servo ID:006 and ID:008. (2) Servo Mount Plate 3D-printed in Session 4, the servo mount plates support the servo ID:005 attachment to the upper torso frame and form a lower arm section between servo ID:006 and ID:007.","title":"Parts:"},{"location":"hardware/Session05/#tools","text":"PH0 Phillips screwdriver plastic rivet tool","title":"Tools:"},{"location":"hardware/Session05/#procedure","text":"","title":"Procedure:"},{"location":"hardware/Session05/#first-start-3d-printing-the-parts-required-for-future-sessions","text":"Base","title":"First, start 3D-printing the parts required for future sessions:"},{"location":"hardware/Session05/#second-assemble-the-right-arm","text":"Mount a swing bracket to servo ID:005 with (4) 3mm miniature screws, taking care to align the tick-marks on the servo horn, the servo cover, and the swing bracket. Attach servo wheel to right shoulder swing bracket by inserting a long rivet through the bracket and into the back of servo ID:006. Do not install the rivet pin to allow the servo wheel to rotate freely. Mount the servo ID:006 horn to the right shoulder swing bracket with (4) 3mm miniature screws and taking care to align servo horn double tick-marks with the tick-mark on the bracket. Note that in this case the alignment is made with the opposite side of the servo horn. With (3) short rivets, attach a servo mount plate to the back of servo ID:007 with the mount hole rings oriented length-wise to the servo. Connect a 130mm cable to the top-side connector of servo ID:006. Attach servo ID:006 to ID:007 using (4) short rivets in the mount plate while routing the cable connected to servo ID:006 through the space between the servo and the mount plate. Connect cable from servo ID:006 to servo ID:007 on its bottom-side connector. Mount the right elbow swing bracket to servo ID:007 with (4) 3mm miniature screws taking care to align the tick-mark on the servo horn with the tick-mark on the servo cover and the bracket. Attach servo wheel to right elbow swing bracket by inserting a long rivet through the bracket and into the back of servo ID:008. Do not install the rivet pin to allow the servo wheel to rotate freely. Mount the servo ID:008 horn to the right elbow swing bracket with (4) 3mm miniature screws and taking care to align servo horn double tick-marks with the tick-mark on the bracket. Note that in this case the alignment is made with the opposite side of the servo horn. Connect a 130mm cable from the top connector of servo ID:007 to the rear-side connector of servo ID:008. Connect a 130mm cable from the rear-side connector of servo ID:005 to the bottom-side connector of servo ID:006.","title":"Second, assemble the right arm:"},{"location":"hardware/Session05/#next-session-6-assemble-the-left-arm","text":"","title":"Next-&gt; Session 6: Assemble the Left Arm"},{"location":"hardware/Session06/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics gestureBot Construction Guide Session 6: Assemble the Left Arm In this session we will 3D-print the eye and ear components (in an alternate color if desired) while assembling the gestureBot's left arm. Parts: servos labeled ID:009, ID:010, ID:011, and ID:012 programmed in Session 2 (3) 130MM servo cable (16) short plastic rivets (2) long plastic rivets (16) 3mm miniature steel screws (2) Screw Mount Swing Bracket 3D-printed in Session 4, the swing brackets and servo wheels form a shoulder joint between servos ID:009 and ID:010 and an elbow joint between servos ID:011 and ID:012. (2) Servo Wheel 3D-printed in Session 4, the servo wheels support the swing bracket mounts for servo ID:010 and ID:012. (2) Servo Mount Plate 3D-printed in Session 4, the servo mount plates support the servo ID:009 attachment to the upper torso frame and form a lower arm section between servo ID:010 and ID:011. Tools: PH0 Phillips screwdriver plastic rivet tool Procedure: First, start 3D-printing the parts required for future sessions: Eyes (use alternative color plastic if desired) (2) Ear (use alternative color plastic if desired) Second, assemble the left arm: Mount a swing bracket to servo ID:009 with (4) 3mm miniature screws, taking care to align the tick-marks on the servo horn, the servo cover, and the swing bracket. Attach servo wheel to left shoulder swing bracket by inserting a long rivet through the bracket and into the back of servo ID:010. Do not install the rivet pin to allow the servo wheel to rotate freely. Mount the servo ID:010 horn to the left shoulder swing bracket with (4) 3mm miniature screws and taking care to align servo horn double tick-marks with the tick-mark on the bracket. Note that in this case the alignment is made with the opposite side of the servo horn. With (3) short rivets, attach a servo mount plate to the back of servo ID:011 with the mount hole rings oriented length-wise to the servo. Connect a 130mm cable to the top-side connector of servo ID:010. Attach servo ID:010 to ID:011 using (4) short rivets in the mount plate while routing the cable connected to servo ID:010 through the space between the servo and the mount plate. Connect cable from servo ID:010 to servo ID:011 on its bottom-side connector. Mount the left elbow swing bracket to servo ID:011 with (4) 3mm miniature screws taking care to align the tick-mark on the servo horn with the tick-mark on the servo cover and the bracket. Attach servo wheel to left elbow swing bracket by inserting a long rivet through the bracket and into the back of servo ID:012. Do not install the rivet pin to allow the servo wheel to rotate freely. Mount the servo ID:012 horn to the left elbow swing bracket with (4) 3mm miniature screws and taking care to align servo horn double tick-marks with the tick-mark on the bracket. Note that in this case the alignment is made with the opposite side of the servo horn. Connect a 130mm cable from the top connector of servo ID:011 to the rear-side connector of servo ID:012. Connect a 130mm cable from the rear-side connector of servo ID:009 to the bottom-side connector of servo ID:010. Next- Session 7: Attach the Arms and Test the Servos","title":"Session06"},{"location":"hardware/Session06/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"hardware/Session06/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"hardware/Session06/#gesturebot-construction-guide","text":"","title":"gestureBot Construction Guide"},{"location":"hardware/Session06/#session-6-assemble-the-left-arm","text":"In this session we will 3D-print the eye and ear components (in an alternate color if desired) while assembling the gestureBot's left arm.","title":"Session 6: Assemble the Left Arm"},{"location":"hardware/Session06/#parts","text":"servos labeled ID:009, ID:010, ID:011, and ID:012 programmed in Session 2 (3) 130MM servo cable (16) short plastic rivets (2) long plastic rivets (16) 3mm miniature steel screws (2) Screw Mount Swing Bracket 3D-printed in Session 4, the swing brackets and servo wheels form a shoulder joint between servos ID:009 and ID:010 and an elbow joint between servos ID:011 and ID:012. (2) Servo Wheel 3D-printed in Session 4, the servo wheels support the swing bracket mounts for servo ID:010 and ID:012. (2) Servo Mount Plate 3D-printed in Session 4, the servo mount plates support the servo ID:009 attachment to the upper torso frame and form a lower arm section between servo ID:010 and ID:011.","title":"Parts:"},{"location":"hardware/Session06/#tools","text":"PH0 Phillips screwdriver plastic rivet tool","title":"Tools:"},{"location":"hardware/Session06/#procedure","text":"","title":"Procedure:"},{"location":"hardware/Session06/#first-start-3d-printing-the-parts-required-for-future-sessions","text":"Eyes (use alternative color plastic if desired) (2) Ear (use alternative color plastic if desired)","title":"First, start 3D-printing the parts required for future sessions:"},{"location":"hardware/Session06/#second-assemble-the-left-arm","text":"Mount a swing bracket to servo ID:009 with (4) 3mm miniature screws, taking care to align the tick-marks on the servo horn, the servo cover, and the swing bracket. Attach servo wheel to left shoulder swing bracket by inserting a long rivet through the bracket and into the back of servo ID:010. Do not install the rivet pin to allow the servo wheel to rotate freely. Mount the servo ID:010 horn to the left shoulder swing bracket with (4) 3mm miniature screws and taking care to align servo horn double tick-marks with the tick-mark on the bracket. Note that in this case the alignment is made with the opposite side of the servo horn. With (3) short rivets, attach a servo mount plate to the back of servo ID:011 with the mount hole rings oriented length-wise to the servo. Connect a 130mm cable to the top-side connector of servo ID:010. Attach servo ID:010 to ID:011 using (4) short rivets in the mount plate while routing the cable connected to servo ID:010 through the space between the servo and the mount plate. Connect cable from servo ID:010 to servo ID:011 on its bottom-side connector. Mount the left elbow swing bracket to servo ID:011 with (4) 3mm miniature screws taking care to align the tick-mark on the servo horn with the tick-mark on the servo cover and the bracket. Attach servo wheel to left elbow swing bracket by inserting a long rivet through the bracket and into the back of servo ID:012. Do not install the rivet pin to allow the servo wheel to rotate freely. Mount the servo ID:012 horn to the left elbow swing bracket with (4) 3mm miniature screws and taking care to align servo horn double tick-marks with the tick-mark on the bracket. Note that in this case the alignment is made with the opposite side of the servo horn. Connect a 130mm cable from the top connector of servo ID:011 to the rear-side connector of servo ID:012. Connect a 130mm cable from the rear-side connector of servo ID:009 to the bottom-side connector of servo ID:010.","title":"Second, assemble the left arm:"},{"location":"hardware/Session06/#next-session-7-attach-the-arms-and-test-the-servos","text":"","title":"Next-&gt; Session 7: Attach the Arms and Test the Servos"},{"location":"hardware/Session07/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics gestureBot Construction Guide Session 7: Attach the Arms and Test the Servos In this session we will 3D-print the hood component (in an alternate color if desired) while we attach the arms to the torso and test the servos using the Robotis Dynamixel Wizard 2.0 application. Parts: controller electronics assembly completed in Session 1 head and torso completed in Session 4 right arm assembly completed in Session 5 left arm assembly completed in Session 6 (2) 190MM servo cables (24) short plastic rivets (2) Servo Mount Plate 3D-printed in Session 4, the servo mount plates support servos' ID:005 and ID:009 (from the arm assemblies) attachment to the upper torso frame. Base 3D-printed in Session 4, the base supports the entire gestureBot. The tabs on the bottom of the base frame insert into the base to provide a stable platform as well as space to contain the controller electronics and USB3 hub. Tools: plastic rivet tool Procedure: First, start 3D-printing the parts required for future sessions: Hood (use alternative color plastic if desired) Second, attach the arms to the torso: Attach the mount plate to servo ID:005 in the right arm assembly using (4) short rivets. Attach the mount plate to servo ID:009 in the left arm assembly using (4) short rivets. Route the cable connecting servo ID:005 and ID:006 through the space between the bottom-rear rivet mount holes of servo ID:005. Mount the right arm assembly to the upper torso frame using (8) short rivets. Route the cable connecting servo ID:009 and ID:010 through the space between the bottom-rear rivet mount holes of servo ID:009. Mount the left arm assembly to the upper torso frame using (8) short rivets. Connect the (2) 190mm cables to the front-side connectors of servos ID:005 and ID:009. Route the (2) 190mm cables connected to servos ID:005 and ID:009 along with the 130mm cable connected to servo ID:004 down through the hip cover and the square hole in the top of the base frame. With the USB3 hub power cable unplugged, insert the cables connected to servo ID:004, ID:005, and ID:009 into the three open connectors on the servo controller PC board. Place the controller electronics and USB3 hub into the base, with the USB terminal block resting in the right-side slot. Insert the large curved tabs on the bottom of the base frame into the slots on the base, with the open end of the base toward the rear. Plug the power connector into the USB3 Hub and observe all of the servos flash red LED's. Third, test the servos: With the USB3 hub power cable connected, insert the USB3 cable into the PC. On the PC, launch the Dynamixel Wizard 2.0 application. Select the Scan button on the top-level menu and allow the dialog box to complete its scan and close by itself. Observe all twelve of the servo ID's listed in the left-side panel. Test control and movement of each servo by selecting them one-by-one in the left-side list, then using the upper-right-side servo UI to turn torque on, move the servo slightly (approximately plus or minus 5 degrees), and then turn torque back off. Tip: Be cautious when manually commanding the servos to move. The gestureBot assembly will physically limit the servos' rotation due to body part collisions. If a servo is driven into a collision, it may cause damage to the parts or the servo. As a protective feature, the servos may automatically shutdown in these cases and require a reset by unplugging the USB hub from the computer and then the power cable from the USB hub, waiting 15 seconds, then first plugging the hub power and then the USB connection to the PC back in. Also, do not change the servos from 'Joint' to 'Motor' mode. If driven in Motor mode the servos will wind their connection cables causing damage or automatic shutdown. Next- Session 8: Complete the Head","title":"Session07"},{"location":"hardware/Session07/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"hardware/Session07/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"hardware/Session07/#gesturebot-construction-guide","text":"","title":"gestureBot Construction Guide"},{"location":"hardware/Session07/#session-7-attach-the-arms-and-test-the-servos","text":"In this session we will 3D-print the hood component (in an alternate color if desired) while we attach the arms to the torso and test the servos using the Robotis Dynamixel Wizard 2.0 application.","title":"Session 7: Attach the Arms and Test the Servos"},{"location":"hardware/Session07/#parts","text":"controller electronics assembly completed in Session 1 head and torso completed in Session 4 right arm assembly completed in Session 5 left arm assembly completed in Session 6 (2) 190MM servo cables (24) short plastic rivets (2) Servo Mount Plate 3D-printed in Session 4, the servo mount plates support servos' ID:005 and ID:009 (from the arm assemblies) attachment to the upper torso frame. Base 3D-printed in Session 4, the base supports the entire gestureBot. The tabs on the bottom of the base frame insert into the base to provide a stable platform as well as space to contain the controller electronics and USB3 hub.","title":"Parts:"},{"location":"hardware/Session07/#tools","text":"plastic rivet tool","title":"Tools:"},{"location":"hardware/Session07/#procedure","text":"","title":"Procedure:"},{"location":"hardware/Session07/#first-start-3d-printing-the-parts-required-for-future-sessions","text":"Hood (use alternative color plastic if desired)","title":"First, start 3D-printing the parts required for future sessions:"},{"location":"hardware/Session07/#second-attach-the-arms-to-the-torso","text":"Attach the mount plate to servo ID:005 in the right arm assembly using (4) short rivets. Attach the mount plate to servo ID:009 in the left arm assembly using (4) short rivets. Route the cable connecting servo ID:005 and ID:006 through the space between the bottom-rear rivet mount holes of servo ID:005. Mount the right arm assembly to the upper torso frame using (8) short rivets. Route the cable connecting servo ID:009 and ID:010 through the space between the bottom-rear rivet mount holes of servo ID:009. Mount the left arm assembly to the upper torso frame using (8) short rivets. Connect the (2) 190mm cables to the front-side connectors of servos ID:005 and ID:009. Route the (2) 190mm cables connected to servos ID:005 and ID:009 along with the 130mm cable connected to servo ID:004 down through the hip cover and the square hole in the top of the base frame. With the USB3 hub power cable unplugged, insert the cables connected to servo ID:004, ID:005, and ID:009 into the three open connectors on the servo controller PC board. Place the controller electronics and USB3 hub into the base, with the USB terminal block resting in the right-side slot. Insert the large curved tabs on the bottom of the base frame into the slots on the base, with the open end of the base toward the rear. Plug the power connector into the USB3 Hub and observe all of the servos flash red LED's.","title":"Second, attach the arms to the torso:"},{"location":"hardware/Session07/#third-test-the-servos","text":"With the USB3 hub power cable connected, insert the USB3 cable into the PC. On the PC, launch the Dynamixel Wizard 2.0 application. Select the Scan button on the top-level menu and allow the dialog box to complete its scan and close by itself. Observe all twelve of the servo ID's listed in the left-side panel. Test control and movement of each servo by selecting them one-by-one in the left-side list, then using the upper-right-side servo UI to turn torque on, move the servo slightly (approximately plus or minus 5 degrees), and then turn torque back off. Tip: Be cautious when manually commanding the servos to move. The gestureBot assembly will physically limit the servos' rotation due to body part collisions. If a servo is driven into a collision, it may cause damage to the parts or the servo. As a protective feature, the servos may automatically shutdown in these cases and require a reset by unplugging the USB hub from the computer and then the power cable from the USB hub, waiting 15 seconds, then first plugging the hub power and then the USB connection to the PC back in. Also, do not change the servos from 'Joint' to 'Motor' mode. If driven in Motor mode the servos will wind their connection cables causing damage or automatic shutdown.","title":"Third, test the servos:"},{"location":"hardware/Session07/#next-session-8-complete-the-head","text":"","title":"Next-&gt; Session 8: Complete the Head"},{"location":"hardware/Session08/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics gestureBot Construction Guide Session 8: Complete the Head In this session we will 3D-print the hands and arm servo covers while we assemble the the head components. Parts: gestureBot (partially assembled through Session 7) (4) short plastic rivets Hood 3D-printed in Session 7, the hood provides a cosmetic cover for the head structure assembly. Eyes 3D-printed in Session 6, the eyes provide a cosmetic cover addition to the face component. Ear 3D-printed in Session 6, the ears provide cosmetic covers and removable access to the interior of the head assembly without requiring the hood to be removed. Tools: PH0 Phillips screwdriver needle nose pliers Procedure: First, start 3D-printing the parts required for future sessions: (6) Servo Cover (12) Servo Side Cover (2) Hand Second, attach the eyes, hood, and ears: Mount the eyes component by inserting the two posts behind the face into the two holes in the bridge between the eyes. Tip: To avoid breaking the face component, be careful to support the face as you push the eyes onto the posts. Mount the hood by tilting the head assembly forward, sliding the hood onto the head swing bracket and attaching the hood to the bracket with (4) rivets. Tip: Using a pair of needle-nose pliers may be helpful in inserting the rivets. Attach the ears buy snapping each into the holes on the sides of the hood. Inspect the complete head assembly. Next- Session 9: Install the Body Covers","title":"Session08"},{"location":"hardware/Session08/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"hardware/Session08/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"hardware/Session08/#gesturebot-construction-guide","text":"","title":"gestureBot Construction Guide"},{"location":"hardware/Session08/#session-8-complete-the-head","text":"In this session we will 3D-print the hands and arm servo covers while we assemble the the head components.","title":"Session 8: Complete the Head"},{"location":"hardware/Session08/#parts","text":"gestureBot (partially assembled through Session 7) (4) short plastic rivets Hood 3D-printed in Session 7, the hood provides a cosmetic cover for the head structure assembly. Eyes 3D-printed in Session 6, the eyes provide a cosmetic cover addition to the face component. Ear 3D-printed in Session 6, the ears provide cosmetic covers and removable access to the interior of the head assembly without requiring the hood to be removed.","title":"Parts:"},{"location":"hardware/Session08/#tools","text":"PH0 Phillips screwdriver needle nose pliers","title":"Tools:"},{"location":"hardware/Session08/#procedure","text":"","title":"Procedure:"},{"location":"hardware/Session08/#first-start-3d-printing-the-parts-required-for-future-sessions","text":"(6) Servo Cover (12) Servo Side Cover (2) Hand","title":"First, start 3D-printing the parts required for future sessions:"},{"location":"hardware/Session08/#second-attach-the-eyes-hood-and-ears","text":"Mount the eyes component by inserting the two posts behind the face into the two holes in the bridge between the eyes. Tip: To avoid breaking the face component, be careful to support the face as you push the eyes onto the posts. Mount the hood by tilting the head assembly forward, sliding the hood onto the head swing bracket and attaching the hood to the bracket with (4) rivets. Tip: Using a pair of needle-nose pliers may be helpful in inserting the rivets. Attach the ears buy snapping each into the holes on the sides of the hood. Inspect the complete head assembly.","title":"Second, attach the eyes, hood, and ears:"},{"location":"hardware/Session08/#next-session-9-install-the-body-covers","text":"","title":"Next-&gt; Session 9: Install the Body Covers"},{"location":"hardware/Session09/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics gestureBot Construction Guide Session 9: Install the Body Covers In this session we will 3D-print the torso cover components while we attach the hands and the arm servo covers. All of the cover components are designed to be removable without disassembling the gestureBot. At this point the gestureBot is ready to run the Labanotation Suite software applications. The torso covers can be attached as soon as they are finished printing. Parts: (6) Servo Covers (8) Servo Side Covers (4) Screw-side Swing Bracket Covers (4) Rivet-side Swing Bracket Covers (2) Hand Upper Torso Front Cover Upper Torse Back Cover Procedure: First, start 3D-printing the parts required for future sessions: Upper Torso Front Cover Upper Torse Back Cover Second, install the right arm covers and hands: Route the cable connecting servo ID:006 to ID:005 folding downward and between the side servo mounting holes, then slide the cover over the servo. Install the servo ID:006 top-side cover by sliding it over the servo cover's ridges. Install the servo ID:006 bottom-side cover by sliding it over the servo cover's ridges and routing the connection cable between the side cover and the servo wheel. Tip: The bottom side cover may need to be slightly cut down if it interferes with the swing bracket. Route the cable connecting servo ID:006 and ID:007 folding downward and between the bottom servo mounting holes, then slide the cover over the servo. Install the servo ID:007 top-side cover by sliding or snapping it over the servo cover's ridges. Install the servo ID:007 bottom-side cover by sliding or snapping it over the servo cover's ridges and routing the cable connecting servo ID:007 to ID:008 through the gap between the servo mounting holes. Route the cable connecting servo ID:007 to ID:008 folding to downward across the connector and between the side servo mounting holes, then slide the cover over the servo. Install the servo ID:008 hand by sliding over the servo cover's side ridges. Install the servo ID:006 rivet-side swing bracket cover by inserting its post into the long rivet hole. Install the servo ID:008 rivet-side swing bracket cover by inserting its post into the long rivet hole. Install the servo ID:006 screw-side swing bracket cover by inserting its post into the bracket hole. Install the servo ID:008 rivet-side swing bracket cover by inserting its post into the bracket hole. Third, repeat the procedure above installing the servo covers and hands for the left arm: Fourth, install the upper torso covers: Slide the upper torso cover over the gestureBot from front to back and taking care to route the cable connecting the shoulder and upper arm servos through the slot on the side of the cover. Install the upper torso back cover by hooking it over the side ridges on the front cover, taking care to place the offset tab on the cover's \"wings\" on the inside of the front cover, and then snapping it into place. Inspect the completed gestureBot covers. Next- Session 10: Run the Gesture Applications","title":"Session09"},{"location":"hardware/Session09/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"hardware/Session09/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"hardware/Session09/#gesturebot-construction-guide","text":"","title":"gestureBot Construction Guide"},{"location":"hardware/Session09/#session-9-install-the-body-covers","text":"In this session we will 3D-print the torso cover components while we attach the hands and the arm servo covers. All of the cover components are designed to be removable without disassembling the gestureBot. At this point the gestureBot is ready to run the Labanotation Suite software applications. The torso covers can be attached as soon as they are finished printing.","title":"Session 9: Install the Body Covers"},{"location":"hardware/Session09/#parts","text":"(6) Servo Covers (8) Servo Side Covers (4) Screw-side Swing Bracket Covers (4) Rivet-side Swing Bracket Covers (2) Hand Upper Torso Front Cover Upper Torse Back Cover","title":"Parts:"},{"location":"hardware/Session09/#procedure","text":"","title":"Procedure:"},{"location":"hardware/Session09/#first-start-3d-printing-the-parts-required-for-future-sessions","text":"Upper Torso Front Cover Upper Torse Back Cover","title":"First, start 3D-printing the parts required for future sessions:"},{"location":"hardware/Session09/#second-install-the-right-arm-covers-and-hands","text":"Route the cable connecting servo ID:006 to ID:005 folding downward and between the side servo mounting holes, then slide the cover over the servo. Install the servo ID:006 top-side cover by sliding it over the servo cover's ridges. Install the servo ID:006 bottom-side cover by sliding it over the servo cover's ridges and routing the connection cable between the side cover and the servo wheel. Tip: The bottom side cover may need to be slightly cut down if it interferes with the swing bracket. Route the cable connecting servo ID:006 and ID:007 folding downward and between the bottom servo mounting holes, then slide the cover over the servo. Install the servo ID:007 top-side cover by sliding or snapping it over the servo cover's ridges. Install the servo ID:007 bottom-side cover by sliding or snapping it over the servo cover's ridges and routing the cable connecting servo ID:007 to ID:008 through the gap between the servo mounting holes. Route the cable connecting servo ID:007 to ID:008 folding to downward across the connector and between the side servo mounting holes, then slide the cover over the servo. Install the servo ID:008 hand by sliding over the servo cover's side ridges. Install the servo ID:006 rivet-side swing bracket cover by inserting its post into the long rivet hole. Install the servo ID:008 rivet-side swing bracket cover by inserting its post into the long rivet hole. Install the servo ID:006 screw-side swing bracket cover by inserting its post into the bracket hole. Install the servo ID:008 rivet-side swing bracket cover by inserting its post into the bracket hole.","title":"Second, install the right arm covers and hands:"},{"location":"hardware/Session09/#third-repeat-the-procedure-above-installing-the-servo-covers-and-hands-for-the-left-arm","text":"","title":"Third, repeat the procedure above installing the servo covers and hands for the left arm:"},{"location":"hardware/Session09/#fourth-install-the-upper-torso-covers","text":"Slide the upper torso cover over the gestureBot from front to back and taking care to route the cable connecting the shoulder and upper arm servos through the slot on the side of the cover. Install the upper torso back cover by hooking it over the side ridges on the front cover, taking care to place the offset tab on the cover's \"wings\" on the inside of the front cover, and then snapping it into place. Inspect the completed gestureBot covers.","title":"Fourth, install the upper torso covers:"},{"location":"hardware/Session09/#next-session-10-run-the-gesture-applications","text":"","title":"Next-&gt; Session 10: Run the Gesture Applications"},{"location":"hardware/Session10/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics gestureBot Construction Guide Session 10: Run the Labanotation Gesture Applications At this point the gestureBot hardware is complete and ready to run the Labanotation Suite software applications. Parts: Completed gestureBot Windows 10 PC with USB3 port Procedure: First, prep the PC to run applications with Python3.7: Follow the software installation instructions for either the Gesture Service or (if you are in a hurry) gestureBot sample projects included in this repository and verify that the browser-based user-interface (UI) services run successfully on the Windows 10 PC. Plug in the gestureBot's USB3 hub's power supply. Connect the gestureBot's USB cable to the PC. Second, run a script to set servo rotation limits: To protect the gestureBot from being damaged with movement commands that cause collisions or pull cables, we will run the Python code in the sample folder: \\gestureBotDesignKit\\src\\Samples\\gestureBotSetLimits\\ Using Microsoft Visual Code or any text editor, open the file in the repository folder: \\gestureBotDesignKit\\src\\Samples\\gestureBotSetLimits\\main.py In line 50 , change the value to match the serial port noted earlier for your gestureBot. For example: self.comPort = com4 Tip: If needed, the serial port can be looked up in Windows Device Manager as shown at the end of Session 1 . Following is an example run session: C:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Samples\\gestureBotSetLimits python main.py gestureBot: connecting to port 'com4'... ID: 1: setting CW angle limit to -70 (272) ID: 1: setting CCW angle limit to 50 (682) ID: 2: setting CW angle limit to -91 (201) ID: 2: setting CCW angle limit to 91 (821) gestureBot: disconnecting from port 'com4'... Tip: In the future, if you modify your gestureBot and want to set different safety limits, this code provides an example in the run() function starting at line 270 . Third, run the Gesture Service sample application and connect it to the gestureBot: Open the URL http://localhost:8001 in the browser. In the Hardware section of the UI, enter the Port value for your gestureBot and select the Connect button. After connecting, the physical gestureBot will move to the current pose and thereafter track the on-screen virtual gestureBot movements until it is disconnected.","title":"Session10"},{"location":"hardware/Session10/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"hardware/Session10/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"hardware/Session10/#gesturebot-construction-guide","text":"","title":"gestureBot Construction Guide"},{"location":"hardware/Session10/#session-10-run-the-labanotation-gesture-applications","text":"At this point the gestureBot hardware is complete and ready to run the Labanotation Suite software applications.","title":"Session 10: Run the Labanotation Gesture Applications"},{"location":"hardware/Session10/#parts","text":"Completed gestureBot Windows 10 PC with USB3 port","title":"Parts:"},{"location":"hardware/Session10/#procedure","text":"","title":"Procedure:"},{"location":"hardware/Session10/#first-prep-the-pc-to-run-applications-with-python37","text":"Follow the software installation instructions for either the Gesture Service or (if you are in a hurry) gestureBot sample projects included in this repository and verify that the browser-based user-interface (UI) services run successfully on the Windows 10 PC. Plug in the gestureBot's USB3 hub's power supply. Connect the gestureBot's USB cable to the PC.","title":"First, prep the PC to run applications with Python3.7:"},{"location":"hardware/Session10/#second-run-a-script-to-set-servo-rotation-limits","text":"To protect the gestureBot from being damaged with movement commands that cause collisions or pull cables, we will run the Python code in the sample folder: \\gestureBotDesignKit\\src\\Samples\\gestureBotSetLimits\\ Using Microsoft Visual Code or any text editor, open the file in the repository folder: \\gestureBotDesignKit\\src\\Samples\\gestureBotSetLimits\\main.py In line 50 , change the value to match the serial port noted earlier for your gestureBot. For example: self.comPort = com4 Tip: If needed, the serial port can be looked up in Windows Device Manager as shown at the end of Session 1 . Following is an example run session: C:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Samples\\gestureBotSetLimits python main.py gestureBot: connecting to port 'com4'... ID: 1: setting CW angle limit to -70 (272) ID: 1: setting CCW angle limit to 50 (682) ID: 2: setting CW angle limit to -91 (201) ID: 2: setting CCW angle limit to 91 (821) gestureBot: disconnecting from port 'com4'... Tip: In the future, if you modify your gestureBot and want to set different safety limits, this code provides an example in the run() function starting at line 270 .","title":"Second, run a script to set servo rotation limits:"},{"location":"hardware/Session10/#third-run-the-gesture-service-sample-application-and-connect-it-to-the-gesturebot","text":"Open the URL http://localhost:8001 in the browser. In the Hardware section of the UI, enter the Port value for your gestureBot and select the Connect button. After connecting, the physical gestureBot will move to the current pose and thereafter track the on-screen virtual gestureBot movements until it is disconnected.","title":"Third, run the Gesture Service sample application and connect it to the gestureBot:"},{"location":"hardware/3D_Print/","text":"gestureBot 3D-Print Files This folder contains 3D-printable files for building the plastic components of your gestureBot. The models are exported from the Blender file gestureBot.blend in the parent folder. Ultimaker3-compatible component sets are provided in the .gcode format. For other 3D-printers, models of individual robot components are provided in the .stl format. Ultimaker3 Component Sets UM3_gB_Base_Set.gcode (x1) gB_Base UM3_gB_BaseFrame_Set.gcode (x1) gB_BaseFrame UM3_gB_Torso_Set.gcode (x1) GB_UpperTorsoFrame (x3) GB_ServoMountPlate (x1) GB_ServoWheel (X1) GB_SwingBracket_RivetMount UM3_gB_HeadFrame_Set.gcode (x1) gB_SwingBracket_Head (x1) gB_ServoWheel (x1) gB_Neck (x1) gB_FaceFrame UM3_gB_ArmFrame_Set.gcode (x4) gB_SwingBracket_ScrewMount (x4) gB_ServoWheel (x2) gB_ServoMountPlate UM3_gB_Face_Set.gcode (x1) gB_Face (x1) gB_SpeakerFrame UM3_gB_EyesEars_Set.gcode (x1) gB_Eyes (x2) gB_Ear UM3_gB_Hood_Set.gcode (x1) gB_Hood UM3_Hand_Set (x2) gB_Hand UM3_gB_ArmCover_Set (x2) gB_ShoulderCover (x2) gB_ArmCover (x2) gB_ElbowCover UM3_gB_FrontCover_Set (x1) gB_FrontCover UM3_gB_BackCover_Set (x1) gB_BackCover Individual Components gB_Base.stl (x1) gB_BaseFrame.stl (x1) gB_BracketCover_RivetSide.stl (x4) gB_BracketCover_ScrewSide.stl (x4) gB_Ear.stl (x2) gB_Eyes.stl (x1) gB_Face.stl (x1) gB_Hand.stl (x2) gB_HeadFrame.stl (x1) gB_HipCover.stl (x1) gB_Hood.stl (x1) gB_Neck.stl (x1) gB_ServoCover.stl (x6) gB_ServoMountPlate.stl (x5) gB_ServoSideCover.stl (x6) gB_ServoWheel.stl (x6) gB_Speaker.stl (x1) gB_SwingBracket_Head.stl (x1) gB_SwingBracket_Hip.stl (x1) gB_SwingBracket_ScrewMount.stl (x4) gB_UpperTorso_BackCover.stl (x1) gB_UpperTorsoFrame.stl (x1) gB_UpperTorso_FrontCover.stl (x1)","title":"gestureBot 3D-Print Files"},{"location":"hardware/3D_Print/#gesturebot-3d-print-files","text":"This folder contains 3D-printable files for building the plastic components of your gestureBot. The models are exported from the Blender file gestureBot.blend in the parent folder. Ultimaker3-compatible component sets are provided in the .gcode format. For other 3D-printers, models of individual robot components are provided in the .stl format.","title":"gestureBot 3D-Print Files"},{"location":"hardware/3D_Print/#ultimaker3-component-sets","text":"","title":"Ultimaker3 Component Sets"},{"location":"hardware/3D_Print/#um3_gb_base_setgcode","text":"(x1) gB_Base","title":"UM3_gB_Base_Set.gcode"},{"location":"hardware/3D_Print/#um3_gb_baseframe_setgcode","text":"(x1) gB_BaseFrame","title":"UM3_gB_BaseFrame_Set.gcode"},{"location":"hardware/3D_Print/#um3_gb_torso_setgcode","text":"(x1) GB_UpperTorsoFrame (x3) GB_ServoMountPlate (x1) GB_ServoWheel (X1) GB_SwingBracket_RivetMount","title":"UM3_gB_Torso_Set.gcode"},{"location":"hardware/3D_Print/#um3_gb_headframe_setgcode","text":"(x1) gB_SwingBracket_Head (x1) gB_ServoWheel (x1) gB_Neck (x1) gB_FaceFrame","title":"UM3_gB_HeadFrame_Set.gcode"},{"location":"hardware/3D_Print/#um3_gb_armframe_setgcode","text":"(x4) gB_SwingBracket_ScrewMount (x4) gB_ServoWheel (x2) gB_ServoMountPlate","title":"UM3_gB_ArmFrame_Set.gcode"},{"location":"hardware/3D_Print/#um3_gb_face_setgcode","text":"(x1) gB_Face (x1) gB_SpeakerFrame","title":"UM3_gB_Face_Set.gcode"},{"location":"hardware/3D_Print/#um3_gb_eyesears_setgcode","text":"(x1) gB_Eyes (x2) gB_Ear","title":"UM3_gB_EyesEars_Set.gcode"},{"location":"hardware/3D_Print/#um3_gb_hood_setgcode","text":"(x1) gB_Hood","title":"UM3_gB_Hood_Set.gcode"},{"location":"hardware/3D_Print/#um3_hand_set","text":"(x2) gB_Hand","title":"UM3_Hand_Set"},{"location":"hardware/3D_Print/#um3_gb_armcover_set","text":"(x2) gB_ShoulderCover (x2) gB_ArmCover (x2) gB_ElbowCover","title":"UM3_gB_ArmCover_Set"},{"location":"hardware/3D_Print/#um3_gb_frontcover_set","text":"(x1) gB_FrontCover","title":"UM3_gB_FrontCover_Set"},{"location":"hardware/3D_Print/#um3_gb_backcover_set","text":"(x1) gB_BackCover","title":"UM3_gB_BackCover_Set"},{"location":"hardware/3D_Print/#individual-components","text":"gB_Base.stl (x1) gB_BaseFrame.stl (x1) gB_BracketCover_RivetSide.stl (x4) gB_BracketCover_ScrewSide.stl (x4) gB_Ear.stl (x2) gB_Eyes.stl (x1) gB_Face.stl (x1) gB_Hand.stl (x2) gB_HeadFrame.stl (x1) gB_HipCover.stl (x1) gB_Hood.stl (x1) gB_Neck.stl (x1) gB_ServoCover.stl (x6) gB_ServoMountPlate.stl (x5) gB_ServoSideCover.stl (x6) gB_ServoWheel.stl (x6) gB_Speaker.stl (x1) gB_SwingBracket_Head.stl (x1) gB_SwingBracket_Hip.stl (x1) gB_SwingBracket_ScrewMount.stl (x4) gB_UpperTorso_BackCover.stl (x1) gB_UpperTorsoFrame.stl (x1) gB_UpperTorso_FrontCover.stl (x1)","title":"Individual Components"},{"location":"src/","text":"Microsoft Applied Robotics Research Library Labanotation Suite: gestureBot Design Kit Source Code Labanotation The Labanotation folder contains gesture data files expressed in Labanotation and stored in .json format Libraries The Libraries folder contains source code for component software modules used by this project's sample applications. Samples The Samples folder contains example application source code for the gestureBot project. Tools The Tools folder contains utility application source code for the gestureBot project.","title":"[![logo](/MARR_logo.png)Microsoft Applied Robotics Research Library](https://github.com/microsoft/AppliedRoboticsResearchLibrary)"},{"location":"src/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"src/#labanotation-suite-gesturebot-design-kit","text":"","title":"Labanotation Suite: gestureBot Design Kit"},{"location":"src/#source-code","text":"","title":"Source Code"},{"location":"src/#labanotation","text":"The Labanotation folder contains gesture data files expressed in Labanotation and stored in .json format","title":"Labanotation"},{"location":"src/#libraries","text":"The Libraries folder contains source code for component software modules used by this project's sample applications.","title":"Libraries"},{"location":"src/#samples","text":"The Samples folder contains example application source code for the gestureBot project.","title":"Samples"},{"location":"src/#tools","text":"The Tools folder contains utility application source code for the gestureBot project.","title":"Tools"},{"location":"src/Labanotation/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics Gesture Library The folder /src/Labanotation/gestureLibrary/ contains sample gesture data expressed as Labanotation scores in the JSON file format. Valid gesture files in child folders will be picked up by the Labanotation Controller service when it launches and made available to client applications. A complete listing of the database including a video clip of each gesture is provided at the end of this page. Classification and Clustering of Gesture-concept Pairs This Gesture Library is organized around 40 clusters of gesture-concept pairs retrievable by a Gesture Engine component. 6 deictic concepts (me, you, this, that, here, there) 33 expressive theme concepts (hello, many, question, etc.) 1 \"beat\" concept used for idling The clusters were segregated using a method described in the paper: Development and Verification of a Gesture-generating Architecture for Conversational Humanoid Robots: First, the most import word in each of a data-set of 230 English conversational sentences was identified using vector analysis in a low-dimensional space. Next, each of these identified words were manually paired with a corresponding gesture encoded in Labanotation. Finally, performance of the 230 gestures was compared using the geodesic distance between elements of movement across the set of gestures to identify the most similar movement sequences and consolidate the groups into a final set of 40 clusters. In each of the expressive theme clusters, synonyms of the concept label are included in the record. Also, there may be found data files containing variations of the gesture associated with the same concept. These features can allow a Gesture Engine to select alternatives in order to avoid un-natural repetitions of exactly the same gesture. The decisions required to define a gesture database in terms of how many and what types of concepts will be included will also be factors in the qualative performance (life-like realism) of gestures performed by robots. At this time, quantitative analysis resulting in concept groupings or clusters has not delivered a higher quality of lifelike gestures over clusters curated subjectively by human mind and hand. Additionally, interchangeable data volumes must be considered for a Gesture Library that contain gesture sets that at-worst stereotype ethnic or gender character traits and at-best can mimic comfortably familiar or iconic celebrity personalities. In any case, the current state of the art does not provide a reasonable method to define a gesture library. More exploration and discovery is needed before robots can, without procedural programming, effectively select and synchronize gestures with rendered speech to a degree that supports and enhances human-robot-interaction. We hope that this gestureBot Design Kit will be useful and support further research and development in human-robot-interaction. Links to related work in this field: https://arxiv.org/abs/1905.08702 Design of conversational humanoid robot based on hardware independent gesture generation https://hal.archives-ouvertes.fr/hal-03047565/document SRG3: Speech-driven Robot Gesture Generation with GAN https://www.osaka-kyoiku.ac.jp/~challeng/SIG-Challenge-057/SIG-Challenge-057-15.pdf Improving Conditional-GAN using Unrolled-GAN for the Generation of Co-speech Upper Body Gesture https://arxiv.org/abs/2010.06194 Manual Clustering: Labeling the Phrase Set of the Conversation Agent, Rinna Gesture Labanotation in JSON Format An example of the data structure in the JSON files is provided in GestureFormat.json . A Labanotation score contains vertical columns representing specific body parts with notations indicating global positions of those parts in transition over a time-line flowing from bottom to top. In our JSON files, from the Labanotation score we group horizontal sets of body-part movements into poses comprising keyframes that correspond to sections within the file. Table of Concept-Gesture Pairs in Library: Deictic concept name similar words gesture file Labanotation Score Video ======= ============================ ========== ============= =============== me I, my, me deictic me I.json you you, your deictic you d.json this this deictic this.json that that deictic that.json here here deictic here.json there there deictic there.json Expressive Themes concept name similar words gesture file Labanotation Score Video ======= ============================ ========= ============= =============== away away, hurry up, go out away.json bad bad, busy, boring, unusual bad.json come come, it's time, help, withdraw come.json confuse confuse, never heard, puzzle confuse.json contrast contrast, upside down, inside out, change contrast.json disgust hate, don't like, dislike, too much dont_like4.json drink drink, coffee, hangover, beer drink.json go go, walk, work out, jog, deposit go.json good good, sweet, money, free, fine, date, great good.json big big, large, huge big.json goodbye goodbye, bye, leaving, good night, see you goodbye.json happy happy, glad, wake, good, moved, encore, cheers, best happy.json hello hello, hi, welcome, good morning hello.json hungry hungry, eat, dinner hungry.json interesting interesting interesting.json laugh laugh, sing laugh.json lets go let's go, have to, it's time, hurry, hard, why don't we, go first letsgo.json many many, more, crowded many.json nice nice, good, well nice.json nod nod, yes, come, I will, whenever, strong, OK, ready nod.json surprise surprise, mess, waste surprise.json panic panic, passout, ouch panic.json please please, go ahead, would you, could you, will you, take care, clean up, I'd like please.json question May I, did you, do you, what, where, when, who, why, how question.json quit quit, knife, stop, off quit.json say say, said, talk, request say.json shakehead no, so, cannot, back, drunk, cannot keep, tone-deaf, not taking, pale, do not feel well, sick, itchy shake_head.json sleepy sleepy, sleep, asleep, yawn sleepy.json small small, out of dish small.json sorry sorry, check sorry.json thanks thank, thanks thanks.json tired tired, hangover, lie down, drunk, tipsy, loaded, not feeling well tired.json weather weather, sunny, cloudy, windy, rainy, nice day weather.json Beat (Idle) Movements name variations gesture file Labanotation Score Video ======= ============================ ========== ============= =============== beat beat, beat_rotate, beat_2, beat_3 beat.json /src/Labanotation/kinectSuite This folder contains gesture data files expressed as Labanotation scores in the JSON file format. These files were captured using a Kinect sensor and human subjects with the Gesture Authoring Tools included in the Labanotation Suite repository.","title":"Home"},{"location":"src/Labanotation/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"src/Labanotation/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"src/Labanotation/#gesture-library","text":"The folder /src/Labanotation/gestureLibrary/ contains sample gesture data expressed as Labanotation scores in the JSON file format. Valid gesture files in child folders will be picked up by the Labanotation Controller service when it launches and made available to client applications. A complete listing of the database including a video clip of each gesture is provided at the end of this page.","title":"Gesture Library"},{"location":"src/Labanotation/#classification-and-clustering-of-gesture-concept-pairs","text":"This Gesture Library is organized around 40 clusters of gesture-concept pairs retrievable by a Gesture Engine component. 6 deictic concepts (me, you, this, that, here, there) 33 expressive theme concepts (hello, many, question, etc.) 1 \"beat\" concept used for idling The clusters were segregated using a method described in the paper: Development and Verification of a Gesture-generating Architecture for Conversational Humanoid Robots: First, the most import word in each of a data-set of 230 English conversational sentences was identified using vector analysis in a low-dimensional space. Next, each of these identified words were manually paired with a corresponding gesture encoded in Labanotation. Finally, performance of the 230 gestures was compared using the geodesic distance between elements of movement across the set of gestures to identify the most similar movement sequences and consolidate the groups into a final set of 40 clusters. In each of the expressive theme clusters, synonyms of the concept label are included in the record. Also, there may be found data files containing variations of the gesture associated with the same concept. These features can allow a Gesture Engine to select alternatives in order to avoid un-natural repetitions of exactly the same gesture. The decisions required to define a gesture database in terms of how many and what types of concepts will be included will also be factors in the qualative performance (life-like realism) of gestures performed by robots. At this time, quantitative analysis resulting in concept groupings or clusters has not delivered a higher quality of lifelike gestures over clusters curated subjectively by human mind and hand. Additionally, interchangeable data volumes must be considered for a Gesture Library that contain gesture sets that at-worst stereotype ethnic or gender character traits and at-best can mimic comfortably familiar or iconic celebrity personalities. In any case, the current state of the art does not provide a reasonable method to define a gesture library. More exploration and discovery is needed before robots can, without procedural programming, effectively select and synchronize gestures with rendered speech to a degree that supports and enhances human-robot-interaction. We hope that this gestureBot Design Kit will be useful and support further research and development in human-robot-interaction. Links to related work in this field: https://arxiv.org/abs/1905.08702 Design of conversational humanoid robot based on hardware independent gesture generation https://hal.archives-ouvertes.fr/hal-03047565/document SRG3: Speech-driven Robot Gesture Generation with GAN https://www.osaka-kyoiku.ac.jp/~challeng/SIG-Challenge-057/SIG-Challenge-057-15.pdf Improving Conditional-GAN using Unrolled-GAN for the Generation of Co-speech Upper Body Gesture https://arxiv.org/abs/2010.06194 Manual Clustering: Labeling the Phrase Set of the Conversation Agent, Rinna","title":"Classification and Clustering of Gesture-concept Pairs"},{"location":"src/Labanotation/#gesture-labanotation-in-json-format","text":"An example of the data structure in the JSON files is provided in GestureFormat.json . A Labanotation score contains vertical columns representing specific body parts with notations indicating global positions of those parts in transition over a time-line flowing from bottom to top. In our JSON files, from the Labanotation score we group horizontal sets of body-part movements into poses comprising keyframes that correspond to sections within the file.","title":"Gesture Labanotation in JSON Format"},{"location":"src/Labanotation/#table-of-concept-gesture-pairs-in-library","text":"","title":"Table of Concept-Gesture Pairs in Library:"},{"location":"src/Labanotation/#deictic","text":"concept name similar words gesture file Labanotation Score Video ======= ============================ ========== ============= =============== me I, my, me deictic me I.json you you, your deictic you d.json this this deictic this.json that that deictic that.json here here deictic here.json there there deictic there.json","title":"Deictic"},{"location":"src/Labanotation/#expressive-themes","text":"concept name similar words gesture file Labanotation Score Video ======= ============================ ========= ============= =============== away away, hurry up, go out away.json bad bad, busy, boring, unusual bad.json come come, it's time, help, withdraw come.json confuse confuse, never heard, puzzle confuse.json contrast contrast, upside down, inside out, change contrast.json disgust hate, don't like, dislike, too much dont_like4.json drink drink, coffee, hangover, beer drink.json go go, walk, work out, jog, deposit go.json good good, sweet, money, free, fine, date, great good.json big big, large, huge big.json goodbye goodbye, bye, leaving, good night, see you goodbye.json happy happy, glad, wake, good, moved, encore, cheers, best happy.json hello hello, hi, welcome, good morning hello.json hungry hungry, eat, dinner hungry.json interesting interesting interesting.json laugh laugh, sing laugh.json lets go let's go, have to, it's time, hurry, hard, why don't we, go first letsgo.json many many, more, crowded many.json nice nice, good, well nice.json nod nod, yes, come, I will, whenever, strong, OK, ready nod.json surprise surprise, mess, waste surprise.json panic panic, passout, ouch panic.json please please, go ahead, would you, could you, will you, take care, clean up, I'd like please.json question May I, did you, do you, what, where, when, who, why, how question.json quit quit, knife, stop, off quit.json say say, said, talk, request say.json shakehead no, so, cannot, back, drunk, cannot keep, tone-deaf, not taking, pale, do not feel well, sick, itchy shake_head.json sleepy sleepy, sleep, asleep, yawn sleepy.json small small, out of dish small.json sorry sorry, check sorry.json thanks thank, thanks thanks.json tired tired, hangover, lie down, drunk, tipsy, loaded, not feeling well tired.json weather weather, sunny, cloudy, windy, rainy, nice day weather.json","title":"Expressive Themes"},{"location":"src/Labanotation/#beat-idle-movements","text":"name variations gesture file Labanotation Score Video ======= ============================ ========== ============= =============== beat beat, beat_rotate, beat_2, beat_3 beat.json","title":"Beat (Idle) Movements"},{"location":"src/Labanotation/#srclabanotationkinectsuite","text":"This folder contains gesture data files expressed as Labanotation scores in the JSON file format. These files were captured using a Kinect sensor and human subjects with the Gesture Authoring Tools included in the Labanotation Suite repository.","title":"/src/Labanotation/kinectSuite"},{"location":"src/Labanotation/kinectSuite/","text":"","title":"Home"},{"location":"src/Libraries/","text":"","title":"Home"},{"location":"src/Samples/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics Software Samples Gesture Service The Gesture Service sample source code in 'gestureService_w2v.pyproj' demonstrates how to instantiate and run all of the software modules in the project together as a system: - Gesture Service - Labanotation controller - Gesture Engine based on Google's word2vec - Gesture Library - gestureBot controller gestureBot The gestureBot sample source code in 'gestureBot.pyproj' demonstrates how to instantiate and run the minimum software modules required to control a physical robot and have it perform gestures from ghe Gesture Library: - Gesture Service - Labanotation controller - Gesture Library - gestureBot controller Simple The sample source code in 'Simple.pyproj' demonstrates how to instantiate and run the Labanotation Controller software module by itself. gestureBotSetLimits The sample source code in 'gestureBotSetLimits.pyproj' demonstrates how to send configuration commands to the servos using the Robotis Dynamixel 2.0 Protocol. This sample is provided as part of the physical gestureBot construction procedures to set specific rotation limits on particular servos. These limits can help prevent damage caused by incorrect commands or code errors resulting in physical collisions of the gestureBot body components. To run this sample, a complete physical gestureBot robot connected to the PC's USB port is required.","title":"Home"},{"location":"src/Samples/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"src/Samples/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"src/Samples/#software-samples","text":"","title":"Software Samples"},{"location":"src/Samples/#gesture-service","text":"The Gesture Service sample source code in 'gestureService_w2v.pyproj' demonstrates how to instantiate and run all of the software modules in the project together as a system: - Gesture Service - Labanotation controller - Gesture Engine based on Google's word2vec - Gesture Library - gestureBot controller","title":"Gesture Service"},{"location":"src/Samples/#gesturebot","text":"The gestureBot sample source code in 'gestureBot.pyproj' demonstrates how to instantiate and run the minimum software modules required to control a physical robot and have it perform gestures from ghe Gesture Library: - Gesture Service - Labanotation controller - Gesture Library - gestureBot controller","title":"gestureBot"},{"location":"src/Samples/#simple","text":"The sample source code in 'Simple.pyproj' demonstrates how to instantiate and run the Labanotation Controller software module by itself.","title":"Simple"},{"location":"src/Samples/#gesturebotsetlimits","text":"The sample source code in 'gestureBotSetLimits.pyproj' demonstrates how to send configuration commands to the servos using the Robotis Dynamixel 2.0 Protocol. This sample is provided as part of the physical gestureBot construction procedures to set specific rotation limits on particular servos. These limits can help prevent damage caused by incorrect commands or code errors resulting in physical collisions of the gestureBot body components. To run this sample, a complete physical gestureBot robot connected to the PC's USB port is required.","title":"gestureBotSetLimits"},{"location":"src/Samples/Simple/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics Simple Sample The sample source code in 'Simple.pyproj' demonstrates how to instantiate and run the Labanotation Controller software module by itself. Software Installation The sample 'Simple' source code depends on a number of open-source Python libraries. This section provides instructions for installing and operating the software. Tested System Software We used the following software versions to test the Gesture Service samples: - Windows 10 (Version 2004, 64-bit) or Linux (Ubuntu18.04, 64-bit) - Microsoft Edge Browser (Version 87.0.664.66, 64-bit) - Git client (Comes with Microsoft Visual Studio Code) - Python 3.7.8 Python Modules The following modules are listed in a requirements.txt file for easy installation: numpy==1.19.3 scipy==1.5.2 tornado==4.5.2 opencv-python==4.4.0.46 pyserial==3.4 msgpack-rpc-python==0.4.1 gensim==3.8.3 nltk==3.5 dynamixel_sdk==3.7.31 Installation Instructions For Windows or Linux, the following instructions will guide you through the installation of code and assets comprising the gestureBot Design Kit as well as dependent external software. If not already on your PC, download and run the following installers: - Python 3.7.8: https://www.python.org/downloads/release/python-378/ PIP: https://bootstrap.pypa.io/get-pip.py From a cmd.exe , bash, or other terminal shell: - Create a folder for the installation in any convenient location and make it the current directory: mkdir [folder path] cd [folder path]] Clone the repository: git clone --recursive https://github.com/microsoft/gestureBotDesignKit Run these commands to download and install required python software modules: Tip: on some systems, earlier versions of python (such as python 2.7) may already be installed with a need to keep them as they are. In this case, it may be required to set a system variable that creates a path to the new installation with a link such as \"python37\". python get-pip.py python -m pip install -r requirements.txt The following examples assume the repository was downloaded to a folder: c:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\ Following is an example run session preceded by a check to insure the correct version of Python 3.7.8 is invoked: C:\\Users\\robotics python Python 3.7.8 (tags/v3.7.8:4b47a5b6ba, Jun 28 2020, 08:53:46) [MSC v.1916 64 bit (AMD64)] on win32 Type help , copyright , credits or license for more information. quit() cd \\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Samples\\Simple C:\\Users\\dbaum\\source\\github_repos\\gestureBotDesignKit\\src\\Samples\\Simple python main.py Labanotation Sample: Simple v1.00.0178 Http controller started on http://localhost:8000. loaded 'Ges02_balancinghands.total.json': 10 frames. Performance duration: 5.347s Ready. Sample Operations The user-interface (UI) in this sample can be run in a browser window on the local machine with the localhost URL's provided in each section below. Tip: If 'localhost' is not a mapped name on the PC, it can be substituted with the PC's local IP address. Also, remote control can be achieved on the local network segment by using the PC's IP address in a browser on a different PC. Labanotation Controller UI http://localhost:8000","title":"Home"},{"location":"src/Samples/Simple/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"src/Samples/Simple/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"src/Samples/Simple/#simple-sample","text":"The sample source code in 'Simple.pyproj' demonstrates how to instantiate and run the Labanotation Controller software module by itself.","title":"Simple Sample"},{"location":"src/Samples/Simple/#software-installation","text":"The sample 'Simple' source code depends on a number of open-source Python libraries. This section provides instructions for installing and operating the software.","title":"Software Installation"},{"location":"src/Samples/Simple/#tested-system-software","text":"We used the following software versions to test the Gesture Service samples: - Windows 10 (Version 2004, 64-bit) or Linux (Ubuntu18.04, 64-bit) - Microsoft Edge Browser (Version 87.0.664.66, 64-bit) - Git client (Comes with Microsoft Visual Studio Code) - Python 3.7.8","title":"Tested System Software"},{"location":"src/Samples/Simple/#python-modules","text":"The following modules are listed in a requirements.txt file for easy installation: numpy==1.19.3 scipy==1.5.2 tornado==4.5.2 opencv-python==4.4.0.46 pyserial==3.4 msgpack-rpc-python==0.4.1 gensim==3.8.3 nltk==3.5 dynamixel_sdk==3.7.31","title":"Python Modules"},{"location":"src/Samples/Simple/#installation-instructions","text":"For Windows or Linux, the following instructions will guide you through the installation of code and assets comprising the gestureBot Design Kit as well as dependent external software. If not already on your PC, download and run the following installers: - Python 3.7.8: https://www.python.org/downloads/release/python-378/ PIP: https://bootstrap.pypa.io/get-pip.py From a cmd.exe , bash, or other terminal shell: - Create a folder for the installation in any convenient location and make it the current directory: mkdir [folder path] cd [folder path]] Clone the repository: git clone --recursive https://github.com/microsoft/gestureBotDesignKit Run these commands to download and install required python software modules: Tip: on some systems, earlier versions of python (such as python 2.7) may already be installed with a need to keep them as they are. In this case, it may be required to set a system variable that creates a path to the new installation with a link such as \"python37\". python get-pip.py python -m pip install -r requirements.txt The following examples assume the repository was downloaded to a folder: c:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\ Following is an example run session preceded by a check to insure the correct version of Python 3.7.8 is invoked: C:\\Users\\robotics python Python 3.7.8 (tags/v3.7.8:4b47a5b6ba, Jun 28 2020, 08:53:46) [MSC v.1916 64 bit (AMD64)] on win32 Type help , copyright , credits or license for more information. quit() cd \\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Samples\\Simple C:\\Users\\dbaum\\source\\github_repos\\gestureBotDesignKit\\src\\Samples\\Simple python main.py Labanotation Sample: Simple v1.00.0178 Http controller started on http://localhost:8000. loaded 'Ges02_balancinghands.total.json': 10 frames. Performance duration: 5.347s Ready.","title":"Installation Instructions"},{"location":"src/Samples/Simple/#sample-operations","text":"The user-interface (UI) in this sample can be run in a browser window on the local machine with the localhost URL's provided in each section below. Tip: If 'localhost' is not a mapped name on the PC, it can be substituted with the PC's local IP address. Also, remote control can be achieved on the local network segment by using the PC's IP address in a browser on a different PC.","title":"Sample Operations"},{"location":"src/Samples/Simple/#labanotation-controller-ui","text":"http://localhost:8000","title":"Labanotation Controller UI"},{"location":"src/Samples/gestureBot/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics gestureBot Sample The gestureBot sample source code in 'gestureBot.pyproj' demonstrates how to instantiate and run the software modules required to control a physical robot and have it perform gestures from ghe Gesture Library: - Gesture Service - Labanotation controller - Gesture Library - gestureBot controller Note: this sample does not include the Gesture Engine module, which is included in the gestureService_w2v.pyroj sample at src/Samples/gestureService_w2v. Software Installation The sample 'gestureBot' source code depends on a number of open-source Python libraries. This section provides instructions for installing and operating the software. Tested System Software We used the following software versions to test the Gesture Service samples: - Windows 10 (Version 2004, 64-bit) or Linux (Ubuntu18.04, 64-bit) - Microsoft Edge Browser (Version 87.0.664.66, 64-bit) - Git client (Comes with Microsoft Visual Studio Code) - Python 3.7.8 Python Modules The following modules are listed in a requirements.txt file for easy installation: numpy==1.19.3 scipy==1.5.2 tornado==4.5.2 opencv-python==4.4.0.46 pyserial==3.4 msgpack-rpc-python==0.4.1 gensim==3.8.3 nltk==3.5 dynamixel_sdk==3.7.31 Installation Instructions For Windows or Linux, the following instructions will guide you through the installation of code and assets comprising the gestureBot Design Kit as well as dependent external software. If not already on your PC, download and run the following installers: - Python 3.7.8: https://www.python.org/downloads/release/python-378/ PIP: https://bootstrap.pypa.io/get-pip.py From a cmd.exe , bash, or other terminal shell: * Create a folder for the installation in any convenient location and make it the current directory: mkdir [folder path] cd [folder path]] Clone the repository: git clone --recursive https://github.com/microsoft/gestureBotDesignKit Run these commands to download and install required python software modules: Tip: on some systems, earlier versions of python (such as python 2.7) may already be installed with a need to keep them as they are. In this case, it may be required to set a system variable that creates a path to the new installation with a link such as \"python37\". python get-pip.py python -m pip install -r requirements.txt The following examples assume the repository was downloaded to a folder: c:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\ Following is an example launch session preceded by a check to insure the correct version of Python 3.7.8 is invoked: C:\\Users\\robotics python Python 3.7.8 (tags/v3.7.8:4b47a5b6ba, Jun 28 2020, 08:53:46) [MSC v.1916 64 bit (AMD64)] on win32 Type help , copyright , credits or license for more information. quit() cd \\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Samples\\gestureBot C:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Samples\\gestureBot python37 main.py Labanotation Sample: gestureBot v1.00.0178 Http controller started on http://localhost:8000. loaded 'Ges02_balancinghands.total.json': 10 frames. Performance duration: 5.347s gestureBot1: creating key frames with a sampling rate of 0.100s per sample... Http gestureBot1 started on http://localhost:8001. Ready. Sample Operations The two user-interfaces (UI) in this sample can be run in a browser window on the local machine with the localhost URL's provided in each section below. Tip: If 'localhost' is not a mapped name on the PC, it can be substituted with the PC's local IP address. Also, remote control can be achieved on the local network segment by using the PC's IP address in a browser on a different PC. Labanotation Controller UI http://localhost:8000 gestureBot Controller UI http://localhost:8001","title":"Home"},{"location":"src/Samples/gestureBot/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"src/Samples/gestureBot/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"src/Samples/gestureBot/#gesturebot-sample","text":"The gestureBot sample source code in 'gestureBot.pyproj' demonstrates how to instantiate and run the software modules required to control a physical robot and have it perform gestures from ghe Gesture Library: - Gesture Service - Labanotation controller - Gesture Library - gestureBot controller Note: this sample does not include the Gesture Engine module, which is included in the gestureService_w2v.pyroj sample at src/Samples/gestureService_w2v.","title":"gestureBot Sample"},{"location":"src/Samples/gestureBot/#software-installation","text":"The sample 'gestureBot' source code depends on a number of open-source Python libraries. This section provides instructions for installing and operating the software.","title":"Software Installation"},{"location":"src/Samples/gestureBot/#tested-system-software","text":"We used the following software versions to test the Gesture Service samples: - Windows 10 (Version 2004, 64-bit) or Linux (Ubuntu18.04, 64-bit) - Microsoft Edge Browser (Version 87.0.664.66, 64-bit) - Git client (Comes with Microsoft Visual Studio Code) - Python 3.7.8","title":"Tested System Software"},{"location":"src/Samples/gestureBot/#python-modules","text":"The following modules are listed in a requirements.txt file for easy installation: numpy==1.19.3 scipy==1.5.2 tornado==4.5.2 opencv-python==4.4.0.46 pyserial==3.4 msgpack-rpc-python==0.4.1 gensim==3.8.3 nltk==3.5 dynamixel_sdk==3.7.31","title":"Python Modules"},{"location":"src/Samples/gestureBot/#installation-instructions","text":"For Windows or Linux, the following instructions will guide you through the installation of code and assets comprising the gestureBot Design Kit as well as dependent external software. If not already on your PC, download and run the following installers: - Python 3.7.8: https://www.python.org/downloads/release/python-378/ PIP: https://bootstrap.pypa.io/get-pip.py From a cmd.exe , bash, or other terminal shell: * Create a folder for the installation in any convenient location and make it the current directory: mkdir [folder path] cd [folder path]] Clone the repository: git clone --recursive https://github.com/microsoft/gestureBotDesignKit Run these commands to download and install required python software modules: Tip: on some systems, earlier versions of python (such as python 2.7) may already be installed with a need to keep them as they are. In this case, it may be required to set a system variable that creates a path to the new installation with a link such as \"python37\". python get-pip.py python -m pip install -r requirements.txt The following examples assume the repository was downloaded to a folder: c:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\ Following is an example launch session preceded by a check to insure the correct version of Python 3.7.8 is invoked: C:\\Users\\robotics python Python 3.7.8 (tags/v3.7.8:4b47a5b6ba, Jun 28 2020, 08:53:46) [MSC v.1916 64 bit (AMD64)] on win32 Type help , copyright , credits or license for more information. quit() cd \\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Samples\\gestureBot C:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Samples\\gestureBot python37 main.py Labanotation Sample: gestureBot v1.00.0178 Http controller started on http://localhost:8000. loaded 'Ges02_balancinghands.total.json': 10 frames. Performance duration: 5.347s gestureBot1: creating key frames with a sampling rate of 0.100s per sample... Http gestureBot1 started on http://localhost:8001. Ready.","title":"Installation Instructions"},{"location":"src/Samples/gestureBot/#sample-operations","text":"The two user-interfaces (UI) in this sample can be run in a browser window on the local machine with the localhost URL's provided in each section below. Tip: If 'localhost' is not a mapped name on the PC, it can be substituted with the PC's local IP address. Also, remote control can be achieved on the local network segment by using the PC's IP address in a browser on a different PC.","title":"Sample Operations"},{"location":"src/Samples/gestureBot/#labanotation-controller-ui","text":"http://localhost:8000","title":"Labanotation Controller UI"},{"location":"src/Samples/gestureBot/#gesturebot-controller-ui","text":"http://localhost:8001","title":"gestureBot Controller UI"},{"location":"src/Samples/gestureBotSetLimits/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics gestureBotSetLimits Sample The sample source code in 'gestureBotSetLimits.pyproj' demonstrates how to send configuration commands to the servos using the Robotis Dynamixel 2.0 Protocol. This sample is provided as part of the physical gestureBot construction procedures to set specific rotation limits on particular servos. These limits can help prevent damage caused by incorrect commands or code errors resulting in physical collisions of the gestureBot body components. To run this sample, a complete physical gestureBot robot connected to the PC's USB port is required. Software Installation The sample 'gestureBotSetLimites' source code depends on a number of open-source Python libraries. This section provides instructions for installing and operating the software. Tested System Software We used the following software versions to test the Gesture Service samples: - Windows 10 (Version 2004, 64-bit) or Linux (Ubuntu18.04, 64-bit) - Microsoft Edge Browser (Version 87.0.664.66, 64-bit) - Git client (Comes with Microsoft Visual Studio Code) - Python 3.7.8 Python Modules The following modules are listed in a requirements.txt file for easy installation: numpy==1.19.3 scipy==1.5.2 tornado==4.5.2 opencv-python==4.4.0.46 pyserial==3.4 msgpack-rpc-python==0.4.1 gensim==3.8.3 nltk==3.5 dynamixel_sdk==3.7.31 Installation Instructions For Windows or Linux, the following instructions will guide you through the installation of code and assets comprising the gestureBot Design Kit as well as dependent external software. If not already on your PC, download and run the following installers: - Python 3.7.8: https://www.python.org/downloads/release/python-378/ PIP: https://bootstrap.pypa.io/get-pip.py From a cmd.exe , bash, or other terminal shell: - Create a folder for the installation in any convenient location and make it the current directory: mkdir [folder path] cd [folder path]] Clone the repository: git clone --recursive https://github.com/microsoft/gestureBotDesignKit Run these commands to download and install required python software modules: Tip: on some systems, earlier versions of python (such as python 2.7) may already be installed with a need to keep them as they are. In this case, it may be required to set a system variable that creates a path to the new installation with a link such as \"python37\". python get-pip.py python -m pip install -r requirements.txt Using Microsoft Visual Code or any text editor, open the file in the repository folder: \\gestureBotDesignKit\\src\\Samples\\gestureBotSetLimits\\main.py In line 50 , change the value to match the serial port noted earlier for your gestureBot. For example: self.comPort = com4 Tip: If needed, the serial port can be looked up in Windows Device Manager as shown at the end of Session 1 . Sample Operations The following example run session starts with a check that the correct verion of Python (3.7.8) is invoked and assumes the repository was downloaded to a folder: c:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\ The example session: C:\\Users\\robotics python Python 3.7.8 (tags/v3.7.8:4b47a5b6ba, Jun 28 2020, 08:53:46) [MSC v.1916 64 bit (AMD64)] on win32 Type help , copyright , credits or license for more information. quit() cd \\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Samples\\gestureBotSetLimits\\ C:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Samples\\gestureBotSetLimits python main.py gestureBot: connecting to port 'com4'... ID: 1: setting CW angle limit to -70 (272) ID: 1: setting CCW angle limit to 50 (682) ID: 2: setting CW angle limit to -91 (201) ID: 2: setting CCW angle limit to 91 (821) gestureBot: disconnecting from port 'com4'... Tip: In the future, if you modify your gestureBot and want to set different safety limits, this code provides an example in the run() function starting at line 270 .","title":"Home"},{"location":"src/Samples/gestureBotSetLimits/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"src/Samples/gestureBotSetLimits/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"src/Samples/gestureBotSetLimits/#gesturebotsetlimits-sample","text":"The sample source code in 'gestureBotSetLimits.pyproj' demonstrates how to send configuration commands to the servos using the Robotis Dynamixel 2.0 Protocol. This sample is provided as part of the physical gestureBot construction procedures to set specific rotation limits on particular servos. These limits can help prevent damage caused by incorrect commands or code errors resulting in physical collisions of the gestureBot body components. To run this sample, a complete physical gestureBot robot connected to the PC's USB port is required.","title":"gestureBotSetLimits Sample"},{"location":"src/Samples/gestureBotSetLimits/#software-installation","text":"The sample 'gestureBotSetLimites' source code depends on a number of open-source Python libraries. This section provides instructions for installing and operating the software.","title":"Software Installation"},{"location":"src/Samples/gestureBotSetLimits/#tested-system-software","text":"We used the following software versions to test the Gesture Service samples: - Windows 10 (Version 2004, 64-bit) or Linux (Ubuntu18.04, 64-bit) - Microsoft Edge Browser (Version 87.0.664.66, 64-bit) - Git client (Comes with Microsoft Visual Studio Code) - Python 3.7.8","title":"Tested System Software"},{"location":"src/Samples/gestureBotSetLimits/#python-modules","text":"The following modules are listed in a requirements.txt file for easy installation: numpy==1.19.3 scipy==1.5.2 tornado==4.5.2 opencv-python==4.4.0.46 pyserial==3.4 msgpack-rpc-python==0.4.1 gensim==3.8.3 nltk==3.5 dynamixel_sdk==3.7.31","title":"Python Modules"},{"location":"src/Samples/gestureBotSetLimits/#installation-instructions","text":"For Windows or Linux, the following instructions will guide you through the installation of code and assets comprising the gestureBot Design Kit as well as dependent external software. If not already on your PC, download and run the following installers: - Python 3.7.8: https://www.python.org/downloads/release/python-378/ PIP: https://bootstrap.pypa.io/get-pip.py From a cmd.exe , bash, or other terminal shell: - Create a folder for the installation in any convenient location and make it the current directory: mkdir [folder path] cd [folder path]] Clone the repository: git clone --recursive https://github.com/microsoft/gestureBotDesignKit Run these commands to download and install required python software modules: Tip: on some systems, earlier versions of python (such as python 2.7) may already be installed with a need to keep them as they are. In this case, it may be required to set a system variable that creates a path to the new installation with a link such as \"python37\". python get-pip.py python -m pip install -r requirements.txt Using Microsoft Visual Code or any text editor, open the file in the repository folder: \\gestureBotDesignKit\\src\\Samples\\gestureBotSetLimits\\main.py In line 50 , change the value to match the serial port noted earlier for your gestureBot. For example: self.comPort = com4 Tip: If needed, the serial port can be looked up in Windows Device Manager as shown at the end of Session 1 .","title":"Installation Instructions"},{"location":"src/Samples/gestureBotSetLimits/#sample-operations","text":"The following example run session starts with a check that the correct verion of Python (3.7.8) is invoked and assumes the repository was downloaded to a folder: c:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\ The example session: C:\\Users\\robotics python Python 3.7.8 (tags/v3.7.8:4b47a5b6ba, Jun 28 2020, 08:53:46) [MSC v.1916 64 bit (AMD64)] on win32 Type help , copyright , credits or license for more information. quit() cd \\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Samples\\gestureBotSetLimits\\ C:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Samples\\gestureBotSetLimits python main.py gestureBot: connecting to port 'com4'... ID: 1: setting CW angle limit to -70 (272) ID: 1: setting CCW angle limit to 50 (682) ID: 2: setting CW angle limit to -91 (201) ID: 2: setting CCW angle limit to 91 (821) gestureBot: disconnecting from port 'com4'... Tip: In the future, if you modify your gestureBot and want to set different safety limits, this code provides an example in the run() function starting at line 270 .","title":"Sample Operations"},{"location":"src/Samples/gestureService_w2v/","text":"Microsoft Applied Robotics Research Library Open Source Samples for Service Robotics Gesture Service System Sample The Gesture Service sample source code in 'gestureService_w2v.pyproj' demonstrates how to instantiate and run all of the software modules in the project together: - Gesture Service - Labanotation controller - Gesture Engine based on Google word2vec - Gesture Library - gestureBot controller Software Installation The sample 'gestureService_w2v' source code depends on a number of open-source Python libraries. This section provides instructions for installing and operating the software. Tested System Software We used the following software versions to test the Gesture Service samples: - Windows 10 (Version 2004, 64-bit) or Linux (Ubuntu18.04, 64-bit) - Microsoft Edge Browser (Version 87.0.664.66, 64-bit) - Git client (Comes with Microsoft Visual Studio Code) - Python 3.7.8 Python Modules The following modules are listed in a requirements.txt file for easy installation: numpy==1.19.3 scipy==1.5.2 tornado==4.5.2 opencv-python==4.4.0.46 pyserial==3.4 msgpack-rpc-python==0.4.1 gensim==3.8.3 nltk==3.5 dynamixel_sdk==3.7.31 Installation Instructions For Windows or Linux, the following instructions will guide you through the installation of code and assets comprising the gestureBot Design Kit as well as dependent external software. If not already on your PC, download and run the following installers: - Python 3.7.8: https://www.python.org/downloads/release/python-378/ PIP: https://bootstrap.pypa.io/get-pip.py From a cmd.exe , bash, or other terminal shell: - Create a folder for the installation in any convenient location and make it the current directory: mkdir [folder path] cd [folder path]] Clone the repository: git clone --recursive https://github.com/microsoft/gestureBotDesignKit Run these commands to download and install required python software modules: Tip: on some systems, earlier versions of python (such as python 2.7) may already be installed with a need to keep them as they are. In this case, it may be required to set a system variable that creates a path to the new installation with a link such as \"python37\". python get-pip.py python -m pip install -r requirements.txt The following examples assume the repository was downloaded to a folder: c:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\ To provide a sample Gesture Engine implementation, Google's neural network word2vec is used and available at this link: https://code.google.com/archive/p/word2vec/#! To install, download the GoogleNews-vectors-negative300.bin binary file and unzip it into the example Library folder as follows: C:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Libraries\\gestureService_w2v\\GoogleNews-vectors-negative300.bin Run the System Following is an example run session preceded by a check to insure the correct version of Python 3.7.8 is invoked: C:\\Users\\robotics python Python 3.7.8 (tags/v3.7.8:4b47a5b6ba, Jun 28 2020, 08:53:46) [MSC v.1916 64 bit (AMD64)] on win32 Type help , copyright , credits or license for more information. quit() cd \\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Samples\\gestureService_w2v C:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Samples\\gestureService_w2v python main.py Labanotation Sample: gesture Service v1.00.0178 Http controller started on http://localhost:8000. gestureBot: creating key frames with a sampling rate of 0.100s per sample... Http gestureBot started on http://localhost:8001. loading 'C:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Libraries\\gestureService_w2v\\GoogleNews-vectors-negative300.bin'... word vector database took 19.139934062957764 seconds to load... Loading and parsing labanotation gesture dictionary... Http application started on http://localhost:8002. Ready. System Operations The three user-interfaces (UI) in this sample can be run in a browser window on the local machine with the localhost URL's provided in each section below. Tip: If 'localhost' is not a mapped name on the PC, it can be substituted with the PC's local IP address. Also, remote control can be achieved on the local network segment by using the PC's IP address in a browser on a different PC. Labanotation Controller UI http://localhost:8000 Gesture Service UI http://localhost:8002 gestureBot Controller UI http://localhost:8001","title":"Home"},{"location":"src/Samples/gestureService_w2v/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"src/Samples/gestureService_w2v/#open-source-samples-for-service-robotics","text":"","title":"Open Source Samples for Service Robotics"},{"location":"src/Samples/gestureService_w2v/#gesture-service-system-sample","text":"The Gesture Service sample source code in 'gestureService_w2v.pyproj' demonstrates how to instantiate and run all of the software modules in the project together: - Gesture Service - Labanotation controller - Gesture Engine based on Google word2vec - Gesture Library - gestureBot controller","title":"Gesture Service System Sample"},{"location":"src/Samples/gestureService_w2v/#software-installation","text":"The sample 'gestureService_w2v' source code depends on a number of open-source Python libraries. This section provides instructions for installing and operating the software.","title":"Software Installation"},{"location":"src/Samples/gestureService_w2v/#tested-system-software","text":"We used the following software versions to test the Gesture Service samples: - Windows 10 (Version 2004, 64-bit) or Linux (Ubuntu18.04, 64-bit) - Microsoft Edge Browser (Version 87.0.664.66, 64-bit) - Git client (Comes with Microsoft Visual Studio Code) - Python 3.7.8","title":"Tested System Software"},{"location":"src/Samples/gestureService_w2v/#python-modules","text":"The following modules are listed in a requirements.txt file for easy installation: numpy==1.19.3 scipy==1.5.2 tornado==4.5.2 opencv-python==4.4.0.46 pyserial==3.4 msgpack-rpc-python==0.4.1 gensim==3.8.3 nltk==3.5 dynamixel_sdk==3.7.31","title":"Python Modules"},{"location":"src/Samples/gestureService_w2v/#installation-instructions","text":"For Windows or Linux, the following instructions will guide you through the installation of code and assets comprising the gestureBot Design Kit as well as dependent external software. If not already on your PC, download and run the following installers: - Python 3.7.8: https://www.python.org/downloads/release/python-378/ PIP: https://bootstrap.pypa.io/get-pip.py From a cmd.exe , bash, or other terminal shell: - Create a folder for the installation in any convenient location and make it the current directory: mkdir [folder path] cd [folder path]] Clone the repository: git clone --recursive https://github.com/microsoft/gestureBotDesignKit Run these commands to download and install required python software modules: Tip: on some systems, earlier versions of python (such as python 2.7) may already be installed with a need to keep them as they are. In this case, it may be required to set a system variable that creates a path to the new installation with a link such as \"python37\". python get-pip.py python -m pip install -r requirements.txt The following examples assume the repository was downloaded to a folder: c:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\ To provide a sample Gesture Engine implementation, Google's neural network word2vec is used and available at this link: https://code.google.com/archive/p/word2vec/#! To install, download the GoogleNews-vectors-negative300.bin binary file and unzip it into the example Library folder as follows: C:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Libraries\\gestureService_w2v\\GoogleNews-vectors-negative300.bin","title":"Installation Instructions"},{"location":"src/Samples/gestureService_w2v/#run-the-system","text":"Following is an example run session preceded by a check to insure the correct version of Python 3.7.8 is invoked: C:\\Users\\robotics python Python 3.7.8 (tags/v3.7.8:4b47a5b6ba, Jun 28 2020, 08:53:46) [MSC v.1916 64 bit (AMD64)] on win32 Type help , copyright , credits or license for more information. quit() cd \\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Samples\\gestureService_w2v C:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Samples\\gestureService_w2v python main.py Labanotation Sample: gesture Service v1.00.0178 Http controller started on http://localhost:8000. gestureBot: creating key frames with a sampling rate of 0.100s per sample... Http gestureBot started on http://localhost:8001. loading 'C:\\Users\\robotics\\github_repos\\gestureBotDesignKit\\src\\Libraries\\gestureService_w2v\\GoogleNews-vectors-negative300.bin'... word vector database took 19.139934062957764 seconds to load... Loading and parsing labanotation gesture dictionary... Http application started on http://localhost:8002. Ready.","title":"Run the System"},{"location":"src/Samples/gestureService_w2v/#system-operations","text":"The three user-interfaces (UI) in this sample can be run in a browser window on the local machine with the localhost URL's provided in each section below. Tip: If 'localhost' is not a mapped name on the PC, it can be substituted with the PC's local IP address. Also, remote control can be achieved on the local network segment by using the PC's IP address in a browser on a different PC.","title":"System Operations"},{"location":"src/Samples/gestureService_w2v/#labanotation-controller-ui","text":"http://localhost:8000","title":"Labanotation Controller UI"},{"location":"src/Samples/gestureService_w2v/#gesture-service-ui","text":"http://localhost:8002","title":"Gesture Service UI"},{"location":"src/Samples/gestureService_w2v/#gesturebot-controller-ui","text":"http://localhost:8001","title":"gestureBot Controller UI"},{"location":"src/Tools/","text":"Microsoft Applied Robotics Research Library Labanotation Suite: gestureBot Design Kit Tools convert The convert tool will take a JSON Labanotation file created with the Gesture Authoring Tools included in the Labanotation Suite repository as input and convert it a JSON file compatible with the software in this repository. usage: main.py [-h] --input INPUT --outputpath OUTPUTPATH lab2png The **lab2png tool will take a Labanotation gesture file in JSON format as input and render a Labanotation score in the .png graphics format as output. usage: main.py [-h] --input INPUT [--width WIDTH]","title":"[![logo](/MARR_logo.png)Microsoft Applied Robotics Research Library](https://github.com/microsoft/AppliedRoboticsResearchLibrary)"},{"location":"src/Tools/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"src/Tools/#labanotation-suite-gesturebot-design-kit","text":"","title":"Labanotation Suite: gestureBot Design Kit"},{"location":"src/Tools/#tools","text":"","title":"Tools"},{"location":"src/Tools/#convert","text":"The convert tool will take a JSON Labanotation file created with the Gesture Authoring Tools included in the Labanotation Suite repository as input and convert it a JSON file compatible with the software in this repository. usage: main.py [-h] --input INPUT --outputpath OUTPUTPATH","title":"convert"},{"location":"src/Tools/#lab2png","text":"The **lab2png tool will take a Labanotation gesture file in JSON format as input and render a Labanotation score in the .png graphics format as output. usage: main.py [-h] --input INPUT [--width WIDTH]","title":"lab2png"},{"location":"src/Tools/convert/","text":"Microsoft Applied Robotics Research Library Labanotation Suite: gestureBot Design Kit convert The convert tool will take a JSON Labanotation file created with the Gesture Authoring Tools included in the Labanotation Suite repository as input and convert it a JSON file compatible with the software in this repository. usage: main.py [-h] --input INPUT --outputpath OUTPUTPATH","title":"[![logo](/MARR_logo.png)Microsoft Applied Robotics Research Library](https://github.com/microsoft/AppliedRoboticsResearchLibrary)"},{"location":"src/Tools/convert/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"src/Tools/convert/#labanotation-suite-gesturebot-design-kit","text":"","title":"Labanotation Suite: gestureBot Design Kit"},{"location":"src/Tools/convert/#convert","text":"The convert tool will take a JSON Labanotation file created with the Gesture Authoring Tools included in the Labanotation Suite repository as input and convert it a JSON file compatible with the software in this repository. usage: main.py [-h] --input INPUT --outputpath OUTPUTPATH","title":"convert"},{"location":"src/Tools/lab2png/","text":"Microsoft Applied Robotics Research Library Labanotation Suite: gestureBot Design Kit lab2png The **lab2png tool will take a Labanotation gesture file in JSON format as input and render a Labanotation score in the .png graphics format as output. usage: main.py [-h] --input INPUT [--width WIDTH]","title":"[![logo](/MARR_logo.png)Microsoft Applied Robotics Research Library](https://github.com/microsoft/AppliedRoboticsResearchLibrary)"},{"location":"src/Tools/lab2png/#microsoft-applied-robotics-research-library","text":"","title":"Microsoft Applied Robotics Research Library"},{"location":"src/Tools/lab2png/#labanotation-suite-gesturebot-design-kit","text":"","title":"Labanotation Suite: gestureBot Design Kit"},{"location":"src/Tools/lab2png/#lab2png","text":"The **lab2png tool will take a Labanotation gesture file in JSON format as input and render a Labanotation score in the .png graphics format as output. usage: main.py [-h] --input INPUT [--width WIDTH]","title":"lab2png"}]}